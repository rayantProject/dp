# Dossier Professionnel

## I. Pr√©sentation de l'environnement de travail

### A. Pr√©sentation de l'entreprise

#### 1. Le groupe Econocom : acteur majeur de la transformation num√©rique

**Econocom** est un groupe europ√©en majeur du num√©rique fond√© en 1974 par Jean-Louis Bouchard (sous le nom ¬´ Europe Computer Syst√®mes ¬ª). Sp√©cialis√© dans les services li√©s √† la transformation num√©rique, il s'est d√©velopp√© autour d'un mod√®le original couvrant l'√©quipement IT, les services et le financement associ√©s. Cot√©e en bourse, l'entreprise se pr√©sente comme ¬´ **la premi√®re entreprise g√©n√©rale du digital en Europe** ¬ª, capable de prendre en charge l'ensemble de la cha√Æne de valeur d'un projet num√©rique (mat√©riel informatique, prestations de service et financement).

Aujourd'hui Econocom revendique environ **2,7 milliards d'euros de chiffre d'affaires** (2024) et **8 450 collaborateurs** dans le monde, avec un r√©seau pr√©sent dans **16 pays**. Le groupe intervient dans de nombreux secteurs d'activit√© (√©ducation, sant√©, industrie, distribution, √©nergie, etc.), ce qui lui conf√®re un positionnement transversal sur le march√© du num√©rique et de l'informatique d'entreprise. En France, par exemple, Econocom est reconnu comme **1er infog√©reur d'environnements utilisateurs** en 2022, mobilisant quelque 5 600 salari√©s d√©di√©s aux services IT, ce qui illustre sa taille et son importance sur le segment des services num√©riques.

#### 2. L'agence Econocom de Lyon-Villeurbanne

Au sein de ce groupe, l'**agence Lyon** (bas√©e √† Villeurbanne) joue le r√¥le d'antenne r√©gionale. Elle dessert le bassin d'entreprises et d'administrations de la m√©tropole lyonnaise, en relais des orientations nationales du groupe. Selon le site officiel, l'adresse de cette agence est ¬´ **13 bis avenue Albert Einstein, 69100 Villeurbanne** ¬ª, t√©moignant de son ancrage local. Cette filiale r√©gionale fait donc partie du maillage territorial d'Econocom en France, permettant au groupe de maintenir une proximit√© de service et de conseil aupr√®s de ses clients. Elle op√®re dans la continuit√© des activit√©s du groupe, en appliquant localement la strat√©gie nationale de transformation digitale et en apportant des solutions adapt√©es aux enjeux des entreprises r√©gionales.

#### 3. Environnement technologique et domaines d'intervention

L'agence de Villeurbanne s'inscrit dans l'√©cosyst√®me technologique d'Econocom, caract√©ris√© par un vaste panel d'expertises IT. La documentation d'Econocom d√©taille **quatre grands domaines d'intervention principaux** :

- **Environnements utilisateurs (Digital Workplace)** : gestion et support du parc informatique des utilisateurs finaux (postes de travail, mobilit√©, assistances bureautique), afin d'assurer une exp√©rience int√©gr√©e et fluide.

- **Cloud et Infrastructures hybrides** : conception, d√©ploiement et exploitation d'infrastructures IT modernes, incluant les solutions cloud (priv√©, public et hybrides), les datacenters et la mise en r√©seau.

- **Applications et donn√©es** : modernisation des applications m√©tier, d√©veloppement de nouvelles solutions logicielles et valorisation des donn√©es (big data, BI, IA), afin d'acc√©l√©rer la transformation num√©rique des processus.

- **Cybers√©curit√©** : mise en place de dispositifs de s√©curit√© et de conformit√© pour anticiper les risques et prot√©ger les syst√®mes d'information et les donn√©es sensibles contre les menaces (audit, pare-feu, SOC, etc.).

Ces domaines refl√®tent l'offre de services d'Econocom : l'agence lyonnaise de Villeurbanne assure √† la fois l'**infog√©rance** (support et maintenance du syst√®me d'information), la **gestion de parc** (asset management), ainsi que des projets de migration vers le cloud et de renforcement de la s√©curit√©. Par exemple, Econocom se positionne comme un leader dans l'infog√©rance des environnements utilisateurs en France, ce qui signifie que le site de Villeurbanne est tr√®s probablement engag√© dans des missions de support utilisateurs, de d√©ploiement de postes et de maintenance du parc informatique.

#### 4. Enjeux SI et technologiques du site de Villeurbanne

Le site de Villeurbanne doit relever plusieurs enjeux typiques de la transformation digitale actuelle :

- **Modernisation des syst√®mes d'information** et du parc informatique : transition vers des environnements hybrides (int√©gration de cloud public/priv√©), d√©ploiement de solutions de productivit√© mobiles et distantes, et optimisation de l'infrastructure r√©seau pour garantir performance et fiabilit√©.

- **Cybers√©curit√©** : mise en ≈ìuvre des bonnes pratiques et outils de protection (gestion des acc√®s, cryptage, supervision continue) pour s√©curiser les infrastructures, d'autant plus qu'Econocom met l'accent sur la s√©curit√© dans ses offres.

- **Num√©rique responsable** : le groupe affiche un engagement fort en mati√®re de num√©rique responsable et se pr√©sente comme "**pionnier de l'√©conomie circulaire du num√©rique**" avec une d√©marche de r√©duction de son empreinte carbone. Concr√®tement, cela se traduit par la gestion durable du cycle de vie des √©quipements (r√©emploi, reconditionnement, recyclage) et l'optimisation √©nerg√©tique des data centers.

- **Mobilit√© et agilit√© du travail** : avec l'essor du t√©l√©travail et des nouveaux usages, le site accompagne les clients dans le d√©ploiement de solutions collaboratives s√©curis√©es (VPN, acc√®s distant, collaboration cloud) et dans l'√©volution vers des modes de travail plus flexibles.

#### 5. Organisation interne et missions

Au sein de l'agence Econocom de Lyon (Villeurbanne), l'organisation interne se structure g√©n√©ralement autour de plusieurs p√¥les : un **p√¥le commercial** (avant-vente et gestion de comptes clients), un **p√¥le support/maintenance** (techniques et exploitations), et un **p√¥le administratif**. La section ¬´ T√©moignages collaborateurs ¬ª du site groupe illustre la diversit√© des profils pr√©sents √† Lyon ‚Äì on y retrouve par exemple des techniciens de support (¬´ gestionnaire SAV Helpdesk ¬ª) et un responsable d'exploitation (¬´ manager des services Surveillance et Exploitation ¬ª).

Parfait, ta partie **Pr√©sentation de l‚Äôentreprise** est d√©j√† tr√®s solide üëå

Pour l‚Äôenrichir avec ton **client Grand Lyon** et la **cartographie de son SI**, je te propose d‚Äôajouter une sous-section sp√©cifique apr√®s ton point 5 ¬´ Organisation interne et missions ¬ª. Cela montrera bien que, dans le cadre d‚ÄôEconocom Villeurbanne, tu as √©t√© int√©gr√© √† une √©quipe en charge d‚Äôun client strat√©gique, et que tu ma√Ætrises son contexte. Voici un texte que tu pourrais ins√©rer :

#### 6. Le client Grand Lyon et son syst√®me d‚Äôinformation

Dans le cadre de mon exp√©rience au sein d‚ÄôEconocom Villeurbanne, j‚Äôai int√©gr√© l‚Äô√©quipe en charge de la gestion du syst√®me d‚Äôinformation du **Grand Lyon**, l‚Äôun des clients majeurs de l‚Äôagence. Le Grand Lyon, ou M√©tropole de Lyon, est une collectivit√© territoriale singuli√®re en France : elle exerce √† la fois les comp√©tences d‚Äôune communaut√© urbaine et celles d‚Äôun d√©partement, au service de plus de **1,4 million d‚Äôhabitants** r√©partis sur 59 communes. Ses missions sont tr√®s diversifi√©es : am√©nagement urbain, mobilit√© et transports, logement et urbanisme, d√©veloppement √©conomique, solidarit√© et sant√© publique, environnement et transition √©cologique, culture, ainsi que les services num√©riques aux citoyens.

Pour soutenir ces politiques publiques, la M√©tropole s‚Äôappuie sur une **Direction de l‚ÄôInnovation Num√©rique et des Syst√®mes d‚ÄôInformation (DINSI)**, qui constitue la cheville ouvri√®re du syst√®me d‚Äôinformation m√©tropolitain. L‚Äôorganigramme de la DINSI met en √©vidence une organisation structur√©e autour de plusieurs grands p√¥les :

* **P√¥le Architecture et Urbanisation du SI**, charg√© de d√©finir les normes, r√©f√©rentiels et principes d‚Äôurbanisation.
* **P√¥le Usages num√©riques**, qui d√©veloppe et maintient les applications m√©tiers, les portails citoyens et les services digitaux.
* **P√¥le Donn√©es et G√©omatique**, sp√©cialis√© dans la gestion et la valorisation des donn√©es territoriales (statistiques, cartographie SIG, open data).
* **P√¥le Infrastructures et R√©seaux**, garant de l‚Äôexploitation des datacenters, de la connectivit√© et des services cloud.
* **P√¥le S√©curit√© des Syst√®mes d‚ÄôInformation**, qui veille √† la conformit√©, √† la r√©silience et √† la cybers√©curit√© du patrimoine num√©rique.
* **P√¥le Support et Digital Workplace**, en charge des environnements utilisateurs (postes de travail, outils collaboratifs, mobilit√©).

![figure1: Cartographie compl√®te du SI du Grand Lyon](../assets/cartographie_compl√®te_du_SI_du_GrandLyon.svg)

Cette organisation illustre bien la cartographie du SI du Grand Lyon, qui peut √™tre d√©crite selon les quatre niveaux classiques :

* **Niveau m√©tier** : r√©pondre aux besoins des politiques publiques m√©tropolitaines (mobilit√©, urbanisme, solidarit√©, environnement, √©conomie).
* **Niveau fonctionnel** : structurer les grands domaines de services (applications m√©tiers, infrastructures, donn√©es, s√©curit√©, support utilisateurs).
* **Niveau applicatif** : g√©rer un patrimoine applicatif riche, incluant les logiciels de gestion financi√®re et RH, les outils d‚Äôurbanisme et de mobilit√©, les portails citoyens, les syst√®mes d‚Äôinformation g√©ographique (SIG), et des plateformes de donn√©es d√©cisionnelles.
* **Niveau infrastructure** : exploiter des datacenters hybrides, des r√©seaux s√©curis√©s interconnectant les sites m√©tropolitains, des environnements cloud, ainsi qu‚Äôun parc utilisateurs de plusieurs milliers de postes, assorti de m√©canismes de supervision, de sauvegarde et de continuit√© d‚Äôactivit√©.

Travailler dans ce contexte a constitu√© une exp√©rience particuli√®rement formatrice. D‚Äôune part, cela m‚Äôa permis de comprendre la complexit√© d‚Äôun SI de grande collectivit√©, marqu√© par la diversit√© des m√©tiers et la multiplicit√© des applications. D‚Äôautre part, cela m‚Äôa confront√© √† des enjeux strat√©giques majeurs : **interop√©rabilit√© des syst√®mes, gouvernance des donn√©es, cybers√©curit√©, Green IT, continuit√© des services publics**. L‚Äô√©quipe Econocom, int√©gr√©e dans ce dispositif, joue un r√¥le de partenaire cl√© en accompagnant la DINSI dans l‚Äôexploitation, la s√©curisation et l‚Äô√©volution de son syst√®me d‚Äôinformation.

#### 7. IA de floutage pour le projet Nestor

##### 1.1 Objectifs du projet

Ce projet de preuve de concept (POC) a √©t√© r√©alis√© dans le cadre de ma mission au sein d‚ÄôEconocom Villeurbanne, prestataire de services num√©riques, pour le compte de son client la M√©tropole de Lyon (Grand Lyon). Ce dernier constitue une collectivit√© territoriale majeure en France, regroupant 59 communes et plus de 1,4 million d‚Äôhabitants. Sa Direction de l‚ÄôInnovation Num√©rique et des Syst√®mes d‚ÄôInformation (DINSI) pilote un syst√®me d‚Äôinformation complexe destin√© √† accompagner de multiples missions : urbanisme, mobilit√©, action sociale, environnement, d√©veloppement √©conomique et services aux citoyens.
Dans ce contexte, la M√©tropole est confront√©e √† des besoins croissants en mati√®re de protection des donn√©es personnelles et de conformit√© r√©glementaire (RGPD, droit √† l‚Äôimage). La gestion de photos et de vid√©os issues de dispositifs urbains (√©v√©nements publics, transport, am√©nagement) pose en effet la question de l‚Äôanonymisation des visages et des plaques d‚Äôimmatriculation avant diffusion ou exploitation.

Le projet **Nestor IA ‚Äì POC Floutage** a pour vocation de d√©montrer la faisabilit√© d'un **syst√®me de floutage automatique d'images** par intelligence artificielle. Il s'agit d'une *preuve de concept* visant √† **d√©tecter puis flouter** des √©l√©ments sensibles sur des images, en particulier les *visages humains* et les *plaques d'immatriculation*, de mani√®re enti√®rement automatis√©e. L'outil r√©pond √† un besoin de **protection de la vie priv√©e** et de *conformit√© r√©glementaire* (par exemple, anonymisation de personnes ou de donn√©es visuelles dans des photos/vid√©os). Ce POC s'inscrit dans un contexte d'**√©valuation technologique** : il permet de comparer **deux approches techniques** (une impl√©mentation en *Python* et une en *.NET*) pour int√©grer cette fonctionnalit√©, afin d'orienter les choix futurs selon les performances et la facilit√© d'int√©gration dans le syst√®me d'information cible.

##### 1.2 Fonctionnalit√©s principales

- **Chargement et traitement d'images**

L'**application Web** permet √† l'utilisateur de t√©l√©verser une image (photo de personnes, de v√©hicules, etc.) via une *interface conviviale*.

- **D√©tection des zones sensibles**

Une fois l'image envoy√©e, le syst√®me identifie automatiquement les **zones d'int√©r√™t √† traiter** ‚Äì typiquement les *visages humains* pr√©sents et/ou les *plaques d'immatriculation* des v√©hicules sur l'image. Ces √©l√©ments sont rep√©r√©s gr√¢ce √† des **mod√®les de d√©tection entra√Æn√©s** (mod√®les de type **YOLO** sp√©cialis√©s pour les visages et pour les plaques).

- **Floutage automatique**

Apr√®s d√©tection, l'application peut appliquer un **floutage** sur les zones sensibles identifi√©es (visages, plaques). L'image renvoy√©e √† l'utilisateur comporte donc ces r√©gions flout√©es, masquant les *informations d'identit√©* ou d'*immatriculation*.

- **Mode d√©tection vs floutage**

Selon la configuration ou le besoin, le syst√®me peut soit fournir un r√©sultat avec **floutage** (image anonymis√©e), soit simplement remonter les *informations de d√©tection* (par exemple pour afficher des cadres ou indiquer les positions des visages/plaque sans alt√©rer l'image originale). Cela permet de valider l'**√©tape de d√©tection** ind√©pendamment du floutage.

- **Interface de visualisation**

Le client web affiche le **r√©sultat** (image flout√©e ou annot√©e) √† l'√©cran. Des indicateurs peuvent √©galement √™tre pr√©sent√©s pour analyse, par exemple le *temps de traitement* de l'image ou le *nombre d'objets d√©tect√©s*. L'interface est con√ßue pour √™tre **simple d'utilisation** afin de servir de d√©monstrateur de la solution.

- **Support multi-backend (Python / .NET)**

Le POC offre **deux impl√©mentations back-end interchangeables** (une en *Python* et une en *C#/.NET*) sans changer le fonctionnement c√¥t√© utilisateur. L'application web interagit de la m√™me fa√ßon avec l'un ou l'autre service. Cela permet de tester les deux technologies offrant les m√™mes fonctionnalit√©s de floutage, et de **comparer leur performance** et comportement dans des conditions identiques.

##### 1.3 Architecture technique

- **Frontend web (Next.js)**

L'interface utilisateur est une **application web** d√©velopp√©e avec **Next.js** (framework React en Node.js/TypeScript). Elle sert une page web permettant de charger des images et d'afficher les r√©sultats. Le front-end envoie les images aux services back-end via des **appels HTTP** (API REST) et attend en r√©ponse soit l'image trait√©e soit les donn√©es de d√©tection. L'application Next.js est contenue dans le r√©pertoire `apps/client` du projet (*monorepo*) et peut √™tre d√©marr√©e en mode d√©veloppement via **Yarn**.

- **Backend Python (Flask)**

Un service **RESTful Flask** en Python 3 est d√©di√© au traitement des images c√¥t√© serveur. Il charge des **mod√®les de d√©tection** de type YOLO (√† l'aide de la biblioth√®que *Ultralytics YOLOv8/PyTorch*) pour rep√©rer les visages et plaques sur l'image entrante. Une fois les objets d√©tect√©s, le service utilise des biblioth√®ques d'√©dition d'images (telles que **OpenCV** et **Pillow**) pour appliquer un *flou gaussien* sur les zones cibl√©es. Le service Flask expose notamment une route API (par exemple `/predict`) qui re√ßoit une image (upload√©e en JSON ou en form-data) et retourne l'image modifi√©e. Ce composant back-end est situ√© dans `services/blur-service-flask/` et peut √™tre lanc√© via la commande `yarn dev:flask` (qui d√©marre simultan√©ment le serveur Flask et le client Next.js en d√©veloppement).

- **Backend .NET (C#)**

En parall√®le, le projet comporte un **service web en .NET** (C#) offrant les m√™mes fonctionnalit√©s de d√©tection/floutage c√¥t√© serveur. Il s'agit d'une API construite avec **ASP.NET Core** (ciblant .NET 7/8+). Ce service exploite les mod√®les de d√©tection convertis au format **ONNX** pour rep√©rer visages et plaques. L'inf√©rence est r√©alis√©e via **ONNX Runtime** (permettant d'ex√©cuter les mod√®les de deep learning entra√Æn√©s en Python dans l'environnement .NET, y compris avec *acc√©l√©ration GPU* si disponible). Pour le floutage, la manipulation de l'image se fait √† l'aide de biblioth√®ques graphiques c√¥t√© .NET, notamment **SkiaSharp** (ou SixLabors ImageSharp) pour charger l'image et appliquer un filtre de flou sur les pixels concern√©s. L'API .NET expose des contr√¥leurs REST (par exemple un contr√¥leur `/Detect` pour obtenir les coordonn√©es des objets d√©tect√©s, et un contr√¥leur `/Predict` pour retourner l'image flout√©e). Ce service se trouve dans `services/blur-service-dotnet/` et se lance via `yarn dev:dotnet` (qui d√©marre le serveur .NET en parall√®le du front-end).

- **Conversion des mod√®les (export ONNX)**

Pour unifier l'usage des **mod√®les d'IA** entre les deux backends, le projet inclut un module d'export de mod√®les. Les mod√®les de d√©tection entra√Æn√©s (initialement au format *PyTorch .pt*) sont export√©s en format **ONNX** gr√¢ce √† un script Python. Ce module est organis√© dans `packages/models-onnx-export/` et un script TypeScript (`scripts/modelExportOnnx.ts`) permet de d√©clencher l'export facilement (commande Yarn d√©di√©e). Ainsi, le m√™me mod√®le de d√©tection (par ex. le mod√®le de visage) peut √™tre utilis√© par le service .NET une fois converti, garantissant la **coh√©rence des r√©sultats** entre l'impl√©mentation Python et C#.

- **Structure du projet (monorepo)**

L'architecture du code est organis√©e sous forme de **monorepo** g√©r√© par *Yarn Workspaces*. √Ä la racine, on retrouve notamment: un fichier de configuration global `nestor-ai-config.json` (contenant les param√®tres comme le chemin des mod√®les, les noms de fichiers de mod√®le pour le visage et la plaque, le type de t√¢che ‚Äì d√©tection ou floutage ‚Äì √† effectuer, etc.), un dossier `services/` (qui contient chaque microservice backend s√©par√©ment, pour Flask et pour .NET), un dossier `apps/` (contenant l'application cliente Next.js), et un dossier `scripts/` (contenant des utilitaires en TypeScript pour automatiser certaines actions de d√©veloppement). Cette organisation *modulaire* facilite le d√©veloppement simultan√© des diff√©rentes composantes et le partage de configurations communes.

- **Scripts et pipeline de d√©veloppement**

Plusieurs **scripts Yarn/TypeScript** contribuent √† automatiser le workflow technique. Par exemple, `yarn setup` ex√©cute un script qui cr√©e et configure l'*environnement virtuel Python* (`.venv`) et installe les d√©pendances n√©cessaires au service Flask. D'autres scripts (`serviceFlask.ts`, `serviceDotnet.ts`) permettent de lancer le backend Flask ou .NET accompagn√© du client web, et de surveiller les modifications de code. Le **pipeline d'ex√©cution** typique consiste √† choisir le backend d√©sir√© (Python ou .NET), exporter les mod√®les en ONNX si on utilise .NET, puis d√©marrer le service choisi en parall√®le du front-end. En environnement de d√©veloppement, le front-end interagit en local avec le backend s√©lectionn√© sur les endpoints API pr√©vus. Des *hooks git* (via **Husky**) et des outils de lint/format (**Prettier**, **Black**, etc.) sont √©galement configur√©s pour assurer la qualit√© et la coh√©rence du code sur l'ensemble du projet.

- **Processus de traitement d'une image (workflow)**

Quel que soit le backend utilis√©, le **d√©roulement** est similaire. L'utilisateur charge une image via l'interface Next.js, celle-ci est envoy√©e au serveur (Flask ou .NET) via une *requ√™te API*. Le serveur charge les **mod√®les de d√©tection** de visage et de plaque (dont les chemins sont d√©finis dans la config), puis ex√©cute une *inference* pour trouver les coordonn√©es des zones √† flouter. Une fois les zones identifi√©es, le serveur applique un **floutage** sur ces r√©gions de l'image ‚Äì en pratique un *flou pixelis√©* ou *gaussien* est appliqu√© aux rectangles englobant les visages et/ou plaques d√©tect√©s. L'image r√©sultante est ensuite renvoy√©e au front-end, qui l'affiche √† l'√©cran pour l'utilisateur. Ce **workflow complet** ‚Äì de la soumission de l'image au retour de l'image flout√©e ‚Äì illustre la cha√Æne de traitement mise en place par le POC.

### B. Description des projets MSPR

#### 1. Analyse des donn√©es √©lectorales

#### 1.1 Objectifs du projet

Le projet **analyse\_data\_election** a pour vocation de d√©velopper un **syst√®me complet d‚Äôanalyse et de visualisation des donn√©es √©lectorales fran√ßaises**. Son ambition est de transformer des donn√©es brutes issues de sources publiques (Minist√®re de l‚ÄôInt√©rieur, INSEE, etc.) en **informations exploitables** pour divers acteurs : journalistes, analystes politiques, citoyens ou encore chercheurs.

Le d√©partement du Rh√¥ne a servi de **terrain d‚Äôexp√©rimentation**. Sa diversit√© socio-√©conomique, son poids politique r√©gional et la disponibilit√© de donn√©es pr√©cises en font un √©chantillon repr√©sentatif. L‚Äôobjectif est de d√©gager des **corr√©lations significatives** entre facteurs d√©mographiques (√¢ge, niveau d‚Äô√©ducation, d√©favorisation, pouvoir d‚Äôachat) et comportements √©lectoraux (vote, abstention, participation).

Enfin, le projet vise √©galement √† explorer des **approches pr√©dictives** gr√¢ce aux r√©seaux de neurones artificiels. Ces mod√®les permettent de pr√©dire les r√©sultats futurs en fonction des tendances observ√©es, tout en respectant les contraintes √©thiques et l√©gales li√©es au **RGPD** (anonymisation, conservation limit√©e, usage strictement analytique).

#### 1.2 Fonctionnalit√©s principales

Le syst√®me offre un ensemble de fonctionnalit√©s r√©pondant aux besoins d‚Äôanalyse exploratoire et d√©cisionnelle :

- **Collecte et int√©gration des donn√©es √©lectorales**

  Un pipeline **ETL** (Extract, Transform, Load) assure l‚Äôimportation, le nettoyage et la fusion des donn√©es √©lectorales avec des indicateurs socio-√©conomiques. Les jeux de donn√©es sont harmonis√©s pour permettre des analyses multi-√©chelles (commune, d√©partement, r√©gion).

- **Analyse comparative multi-√©lections**

  L‚Äôapplication permet de comparer les r√©sultats entre diff√©rents scrutins (pr√©sidentielles, l√©gislatives, municipales, europ√©ennes) et de suivre l‚Äô√©volution temporelle des comportements √©lectoraux.

- **Visualisation g√©ographique interactive**

  Les r√©sultats √©lectoraux sont repr√©sent√©s sous forme de **cartes dynamiques**. Les communes ou d√©partements sont color√©s selon les scores ou les taux de participation, ce qui rend imm√©diatement visible l‚Äôimplantation territoriale des partis ou l‚Äôabstention.

- **Tableaux de bord dynamiques**

  Des dashboards interactifs permettent de croiser plusieurs indicateurs via des graphiques (courbes, barres, heatmaps). L‚Äôutilisateur peut filtrer par √¢ge, √©ducation ou pouvoir d‚Äôachat pour mieux comprendre les facteurs influen√ßant le vote.

- **Analyse pr√©dictive et mod√©lisation**

  Un **r√©seau de neurones artificiels** a √©t√© impl√©ment√© pour r√©aliser deux types de t√¢ches :

  - **Classification** : pr√©dire le candidat vainqueur dans une commune.
  - **R√©gression** : estimer les pourcentages obtenus par chaque candidat.
    Le mod√®le, flexible et ajustable (hyperparam√®tres, couches), atteint une pr√©cision de **0,83** sur les donn√©es test, d√©montrant sa fiabilit√©.

- **Export et partage**

  Les r√©sultats d‚Äôanalyses peuvent √™tre export√©s en **PDF, Excel ou images** afin de faciliter leur r√©utilisation et diffusion.

- **Respect du RGPD**

  Le syst√®me applique les principes de protection des donn√©es : anonymisation, usage limit√© √† l‚Äôanalyse, conservation restreinte et s√©curisation.

#### 1.3 Architecture technique

Le projet repose sur une architecture modulaire pens√©e pour la **scalabilit√© et la reproductibilit√©** :

- **Backend de traitement de donn√©es (Python)**

  Le c≈ìur du syst√®me utilise **Pandas** pour la manipulation des donn√©es, **NumPy** pour les calculs num√©riques et **Scikit-learn / TensorFlow-Keras** pour l‚Äôentra√Ænement des mod√®les de machine learning.

- **Base de donn√©es et stockage**

  Les donn√©es √©lectorales sont structur√©es dans une **base relationnelle** (PostgreSQL ou SQLite) avec indexation optimis√©e. Un **data warehouse simplifi√©** permet l‚Äôhistorisation et l‚Äôagr√©gation multi-√©lections.

- **Framework de visualisation**

  L‚Äôinterface est construite avec **Dash** ou **Streamlit**, combinant des visualisations interactives (Plotly, Matplotlib) et des cartes (Geopandas, Folium).

- **API REST et services web**

  Les analyses sont expos√©es via une **API RESTful**. Les endpoints permettent de requ√™ter les r√©sultats √©lectoraux, d‚Äôobtenir des visualisations ou de lancer une pr√©diction.

- **Pipeline ETL automatis√©**

  Les donn√©es sont rafra√Æchies p√©riodiquement gr√¢ce √† des scripts Python int√©gr√©s au pipeline. Chaque mise √† jour inclut un processus de validation et de contr√¥le qualit√©.

- **Interface utilisateur responsive**

  Le frontend est con√ßu pour √™tre utilisable aussi bien sur **desktop que mobile**, afin de rendre l‚Äôexploration accessible au plus grand nombre.

- **S√©curit√© et performance**

  Le syst√®me impl√©mente une **authentification s√©curis√©e**, une gestion des autorisations et des optimisations (cache, pagination, compression) pour garantir des temps de r√©ponse rapides, m√™me avec de gros volumes de donn√©es.

#### 2. Optimisation et am√©lioration du Syst√®me d'Information de K-ElectroniK

##### 2.1 Objectifs du projet

Nous avons entrepris un projet visant √† **optimiser et moderniser** le Syst√®me d'Information (SI) existant de **K-ElectroniK**, une PME fran√ßaise en pleine *expansion internationale*. L'entreprise fait face √† plusieurs enjeux simultan√©s : l'int√©gration d'une **filiale espagnole** r√©cemment acquise, la n√©cessit√© de remplacer ou mettre √† jour des *outils num√©riques vieillissants*, le renforcement de la **s√©curit√© informatique** et de la *gouvernance du SI*, ainsi que la prise en compte des exigences de **responsabilit√© soci√©tale** (Green IT/RSE). Dans ce contexte, un **plan de transformation du SI** a √©t√© valid√© par la direction, et notre √©quipe a √©t√© mandat√©e pour r√©aliser un *diagnostic complet* de l'existant, proposer des **am√©liorations concr√®tes** et accompagner K-ElectroniK vers une meilleure *maturit√© num√©rique*, en alignant les √©volutions du SI sur la **strat√©gie globale** de l'entreprise. L'objectif final du projet est donc de doter K-ElectroniK d'un **SI unifi√©, fiable et √©volutif**, capable de soutenir sa croissance et d'am√©liorer son *efficacit√© op√©rationnelle* tout en r√©duisant les risques (pannes, failles de s√©curit√©) et en r√©pondant aux nouvelles attentes des clients et partenaires.

##### 2.2 Fonctionnalit√©s principales

Pour r√©pondre aux besoins m√©tiers recens√©s et aux probl√®mes identifi√©s, nous avons d√©fini les **fonctionnalit√©s cl√©s** que le nouveau SI ou les √©volutions apport√©es doivent offrir :

- **CRM unifi√© et marketing int√©gr√©**

D√©ploiement complet de l'outil de gestion de la relation client **FreshSales** sur l'ensemble du groupe (France et Espagne), afin d'exploiter tous ses modules (notamment les *campagnes marketing*) et d'unifier la gestion des clients et prospects. Cette √©volution permet de **centraliser les informations client** et de mener des campagnes marketing cibl√©es depuis une plateforme commune.

- **Gestion de production modernis√©e**

Refonte compl√®te du **logiciel de gestion de production** interne (l'outil historique datant de 2005) par une solution moderne. Le nouveau module prendra en charge la *planification de la production*, la *gestion des stocks* et le *suivi des personnalisations produits*, afin de remplacer l'ancien syst√®me obsol√®te et de **gagner en efficacit√©**.

- **Renforcement de la s√©curit√© et continuit√© d'activit√©**

Mise en place d'une **Politique de S√©curit√© des Syst√®mes d'Information (PSSI)** formalis√©e, accompagn√©e de plans de reprise et de continuit√© d'activit√© (**PRA/PCA**). Concr√®tement, nous pr√©voyons l'*authentification multi-facteur (MFA)* sur les acc√®s sensibles, la *sauvegarde automatis√©e* et externalis√©e des donn√©es critiques, et l'application des bonnes pratiques issues de la **norme ISO 27002** afin de structurer la s√©curit√© du SI. Ces mesures visent √† prot√©ger les informations de l'entreprise et √† assurer la **disponibilit√© du SI** m√™me en cas d'incident majeur.

- **Fusion et unification des syst√®mes France/Espagne**

Int√©gration de l'infrastructure et des applications de la **filiale espagnole** avec celles du si√®ge fran√ßais. L'objectif est d'harmoniser les outils et pratiques informatiques entre les deux entit√©s (par exemple en d√©ployant le m√™me *ERP, CRM* et autres logiciels dans les deux pays) afin de supprimer les silos et de r√©duire la **dette technique** accumul√©e historiquement. √Ä terme, cela permettra un *pilotage centralis√©* du SI du groupe et des **donn√©es unifi√©es** au niveau europ√©en.

- **Portail de services IT (ITSM)**

Mise en ≈ìuvre d'un **portail centralis√©** pour le support informatique interne, bas√© sur un outil d'*IT Service Management* (tel que **GLPI** ou **Jira Service Management**). Ce portail permettra aux employ√©s de formuler des demandes ou de signaler des incidents via un *syst√®me de tickets*, ce qui am√©liore le support en centralisant et en suivant les demandes, et formalise les **engagements de service (SLA)** du c√¥t√© de la DSI. L'adoption d'un tel module accro√Æt la *tra√ßabilit√©* des demandes et contribue √† r√©duire les d√©lais de r√©solution gr√¢ce √† une meilleure priorisation.

- **Initiatives "Green IT" et RSE**

Introduction de pratiques d'**informatique responsable** afin de minimiser l'*empreinte carbone* du SI (par exemple en optimisant la consommation √©nerg√©tique des serveurs et en allongeant le cycle de vie du mat√©riel). Nous proposons √©galement de suivre des **indicateurs RSE** li√©s au num√©rique (*bilan carbone du SI, taux de recyclage du mat√©riel*, etc.) et de sensibiliser les utilisateurs aux *√©co-gestes num√©riques*. Ces fonctionnalit√©s transverses visent √† aligner le SI sur les valeurs de **d√©veloppement durable** de l'entreprise et √† r√©pondre aux attentes croissantes des clients en la mati√®re.

##### 2.3 Architecture technique

D'un point de vue technique, l'architecture du SI de K-ElectroniK √©volue d'un **environnement h√©t√©rog√®ne** vers une infrastructure *unifi√©e, moderne et s√©curis√©e*. √Ä l'√©tat initial, le SI comportait un **m√©lange disparate** de technologies et d'applications : certaines fonctions critiques reposent sur des outils d√©velopp√©s en interne il y a plus de 10 ans (ex : un logiciel maison *"GestComs"* pour les achats, un programme sur mesure de 2005 pour piloter la production), tandis que d'autres s'appuient sur des **solutions du march√©** plus r√©centes (par ex. **SmartRH** en SaaS pour la gestion RH, **Cegid** pour la comptabilit√©/finance, **FreshSales CRM** utilis√© de fa√ßon partielle pour le suivi commercial, ou encore **PrestaShop** pour le site e-commerce).

Dans le cadre du projet, nous pr√©conisons de **rationaliser et moderniser** l'architecture logicielle. Le remplacement de l'outil de production obsol√®te sera l'occasion d'adopter une **architecture applicative moderne**, par exemple une *application web multi-couches* (front-end, API, base de donn√©es) afin d'en faciliter la maintenance et l'int√©gration avec les autres syst√®mes. Les nouvelles solutions seront choisies en privil√©giant l'**interop√©rabilit√©** : des interfaces (**API** ou connecteurs) permettront la communication entre le CRM, l'ERP et le site e-commerce, afin d'√©liminer les ressaisies et les *silos de donn√©es*. Par ailleurs, pour am√©liorer le support aux utilisateurs, nous allons d√©ployer un **portail ITSM** bas√© sur une solution open-source √©prouv√©e (type **GLPI**), ce qui structurera la gestion des incidents/demandes et en am√©liorera la tra√ßabilit√©.

Sur le plan de l'infrastructure, nous recommandons l'**unification du r√©seau** entre les sites de Paris et Madrid (*annuaire et authentification centralis√©s*) et la mise en place de m√©canismes robustes de **s√©curit√© et sauvegarde**. Concr√®tement, cela inclut l'activation d'une *authentification forte (MFA)* sur l'ensemble des applications critiques, la mise en place de **sauvegardes automatiques externalis√©es** (en cloud) et d'un **Plan de Reprise d'Activit√©** document√© et test√© r√©guli√®rement. Ces mesures augmenteront la *r√©silience* du syst√®me en cas de sinistre et assureront la disponibilit√© des services cl√©s.

Enfin, l'architecture cible int√®gre des **outils de supervision** et de pilotage du SI, afin de mesurer la performance et d'appuyer la *gouvernance technique*. Nous pr√©voyons par exemple de d√©ployer des **tableaux de bord dynamiques** (via des solutions telles que **Microsoft Power BI** ou **Grafana**) permettant de visualiser en temps r√©el les indicateurs du SI (*√©tat des serveurs, tickets en cours, niveaux de service*, etc.). Ces tableaux de bord offriront un **pilotage r√©actif** du SI, en aidant l'√©quipe informatique √† d√©tecter proactivement les anomalies et √† justifier des ressources ou investissements sur des donn√©es objectives. L'ensemble du code, des configurations et des documents produits sera g√©r√© de fa√ßon professionnelle (*gestion de version, documentation technique, segmentation en modules clairs*) afin de faciliter la maintenance et les futures √©volutions du syst√®me. Ce **socle technique modernis√©** permettra √† K-ElectroniK de disposer d'un SI *agile, s√©curis√©* et align√© sur ses processus m√©tier, tout en √©tant pr√©par√© pour les d√©fis √† venir.

#### 3. Projet Cofrap

##### 3.1 Contexte et finalit√© g√©n√©rale

**Cofrap** (Compagnie Fran√ßaise de R√©alisation d'Applicatifs Professionnels) est une entreprise sp√©cialis√©e dans les **solutions logicielles de gestion d'entreprise**, concurrente de COGIP sur les *ERP et applications web m√©tier*. Face √† la croissance de sa base d'utilisateurs et √† des **compromissions r√©currentes de comptes cloud** (li√©es √† des mots de passe faibles et √† la non-adoption de la 2FA), Cofrap a d√©cid√© de **moderniser son processus d'inscription et d'authentification**. L'objectif est de mettre en place une **infrastructure cloud-native et serverless** (sur Kubernetes) assurant la g√©n√©ration s√©curis√©e de mots de passe complexes, l'activation syst√©matique de la *double authentification (2FA TOTP)* et la distribution des identifiants via **QR codes**. Ce projet (PoC) vise ainsi √† **automatiser et centraliser** la gestion des comptes utilisateurs tout en r√©pondant aux exigences de **s√©curit√©, de performance et de scalabilit√©** de Cofrap.

##### 3.2 Objectifs du projet

- **S√©curit√© renforc√©e**

√âliminer les risques li√©s aux **mots de passe faibles** en imposant une g√©n√©ration automatique de mots de passe complexes.

- **Authentification forte**

Rendre obligatoire l'usage d'une **double authentification TOTP** pour tous les comptes.

- **Distribution simplifi√©e**

Fournir aux utilisateurs des **identifiants** (login, mot de passe, secret 2FA) via des *QR codes √† usage unique*, pour simplifier l'enregistrement.

- **Cycle de vie ma√Ætris√©**

D√©finir une **validit√© limit√©e** des identifiants (6 mois), avec un m√©canisme de *renouvellement automatique* des comptes expir√©s.

- **Interface utilisateur**

Offrir une **interface simple** permettant de cr√©er un compte utilisateur (si inexistant), de s'authentifier (login + mot de passe + code TOTP) et de d√©clencher automatiquement la *remise √† z√©ro* des identifiants expir√©s.

Ces objectifs m√©tier s'inscrivent dans les enjeux de Cofrap : renforcer la **s√©curit√© des acc√®s cloud**, r√©duire les *co√ªts de support* en automatisant la gestion des comptes, et optimiser les ressources techniques via une architecture **serverless (OpenFaaS)** *"scale-to-zero"*.

##### 3.3 Fonctionnalit√©s principales

- **Cr√©ation de compte s√©curis√©**

G√©n√©ration automatique d'un **mot de passe de 24 caract√®res** (avec majuscules, minuscules, chiffres, caract√®res sp√©ciaux) et d'un *secret TOTP chiffr√©s*, stock√©s en base.

- **G√©n√©ration de QR codes**

Cr√©ation de **QR codes √† usage unique** contenant les identifiants (mot de passe et cl√© 2FA) pour simplifier la transmission et l'enregistrement c√¥t√© utilisateur.

- **Double authentification**

Activation obligatoire de l'**authentification TOTP** √† chaque connexion, avec v√©rification du code et de la validit√© de l'identifiant par une *fonction serverless*.

- **Gestion du cycle de vie**

**Expiration des identifiants** au bout de 6 mois et relance automatique du processus de cr√©ation si un utilisateur tente de s'authentifier avec des identifiants p√©rim√©s.

- **Interface front-end simple**

Pages web (cr√©ation de compte, affichage du QR code, formulaire de connexion) permettant √† l'utilisateur de d√©clencher les **appels API** ad√©quats (via *OpenFaaS Gateway*).

##### 3.4 Architecture technique

- **Frontend**

Application web en **React/HTML** (monopage) servant d'interface utilisateur. Le front-end communique avec les fonctions backend via des **requ√™tes HTTP REST** (*OpenFaaS Gateway, faas-netes*).

- **Backend (serverless OpenFaaS)**

**Fonctions** √©crites en **Node.js** d√©ploy√©es sur *OpenFaaS*. Chaque fonction g√®re une t√¢che m√©tier (g√©n√©ration de mot de passe, g√©n√©ration de secret 2FA, v√©rification d'authentification, etc.). Le choix de Node.js facilite l'int√©gration de biblioth√®ques JavaScript pour le *chiffrement*, la *gestion TOTP* et la cr√©ation de *QR codes*. OpenFaaS a √©t√© d√©ploy√© dans un **cluster Kubernetes** (via Helm charts) pour ex√©cuter ces fonctions de mani√®re √©volutive.

- **Base de donn√©es**

**MongoDB** (ou √©quivalent NoSQL) pour stocker les comptes utilisateur (login, mot de passe chiffr√©, secret 2FA, dates d'expiration). L'utilisation de Node.js s'int√®gre naturellement avec MongoDB pour le stockage des *secrets* et des *logs d'authentification*.

- **Orchestration/d√©ploiement**

**Cluster Kubernetes local** (K3s sous OrbStack) pour d√©ployer tous les composants (OpenFaaS, services, base de donn√©es, Traefik/M√©talLB). Le d√©ploiement a √©t√© automatis√© avec **Helm** et la CLI OpenFaaS (*faas-cli*) pour assurer un provisionnement reproductible. Le **pipeline CI/CD** repose sur Docker et *GitHub Actions/GitHub* pour builder les images de fonctions et les d√©ployer dans le cluster. La gestion des secrets est assur√©e par la combinaison de *faas-secrets* et des objets **Secret Kubernetes**.

- **Flux de donn√©es utilisateurs**

**Cr√©ation de compte** : l'utilisateur remplit le formulaire front-end (login souhait√©). Le front appelle une fonction OpenFaaS (ex. `/function/create-account`) qui g√©n√®re le mot de passe complexe et le secret 2FA, les chiffre et les stocke en base, puis retourne un *QR code* √† l'application. Le frontend affiche le QR code pour que l'utilisateur le scanne et enregistre ses identifiants.

**Authentification** : l'utilisateur soumet son login, mot de passe et code TOTP au front, qui appelle la fonction d'authentification. Celle-ci v√©rifie le mot de passe, le code 2FA et la date d'expiration du compte. Si l'utilisateur est expir√©, elle renvoie un statut *¬´ compte expir√© ¬ª*, d√©clenchant automatiquement le front √† red√©clencher la cr√©ation de compte (renouvellement des identifiants).

##### 3.5 M√©thodologie de travail

Le projet a √©t√© men√© en mode **agile**, avec un d√©coupage it√©ratif en *sprints courts* (1 semaine environ) et des rituels **Scrum** simplifi√©s. Chaque membre disposait de **r√¥les d√©finis** (*Scrum Master, Product Owner, d√©veloppeurs backend/frontend, DevOps, r√©f√©rent qualit√©/documentation*). Les t√¢ches (*user stories* et tickets) ont √©t√© g√©r√©es via un **Kanban (Jira)** et un planning de projet (*diagramme de Gantt*). L'√©quipe a utilis√© **GitHub** pour le code (branches, pull requests), ainsi que *Jira/Slack/GitHub* comme tableau Kanban partag√©. Des **points quotidiens** (*stand-up*) permettaient de faire le suivi et de r√©affecter rapidement les t√¢ches. Les outils num√©riques incluaient √©galement *Google Meet/Zoom* pour les r√©unions, *Google Drive/Wiki* pour la documentation, et **Slack** pour la communication asynchrone. Cette organisation agile, alli√©e aux sprints planifi√©s et √† la r√©partition claire des responsabilit√©s, a assur√© la **tra√ßabilit√© du projet** et une *livraison progressive* des fonctionnalit√©s.

##### 3.6 Particularit√©s humaines et manag√©riales

Le projet a √©t√© r√©alis√© dans un **environnement distanci√© et multiculturel**. L'√©quipe a majoritairement travaill√© en *t√©l√©travail*, maintenant une communication fluide par les **outils collaboratifs** (*Slack, Google Meet, Zoom*) et des rituels r√©guliers (*daily meetings, revues de sprint*). √âtant compos√©e de membres d'horizons culturels vari√©s, l'√©quipe a choisi l'**anglais technique** comme langue de travail pour garantir l'*inclusion linguistique* et une compr√©hension commune. L'**accessibilit√©** a √©galement √©t√© prise en compte : l'interface utilisateur a √©t√© con√ßue avec des principes d'accessibilit√© (*navigation clavier, √©l√©ments ARIA, contraste adapt√©*) pour les personnes √† mobilit√© r√©duite ou d√©ficientes visuelles. Sur le plan manag√©rial, un **climat de travail bienveillant** a √©t√© cultiv√©, avec un √©quilibre entre performance et bien-√™tre. L'√©quipe a veill√© √† adapter les charges de travail, offrir un cadre flexible et soutenir les membres en cas de blocages, favorisant ainsi la **motivation et la cr√©ativit√©** de chacun.

##### 3.7 Bilan du projet

Au terme du **PoC Cofrap**, l'√©quipe a nettement progress√© techniquement : elle ma√Ætrise d√©sormais le **d√©ploiement Kubernetes** (OrbStack), la *programmation serverless OpenFaaS*, ainsi que le d√©veloppement **Node.js et MongoDB** dans un contexte *cloud-native*. Les membres ont gagn√© en **autonomie** (gestion de leur temps, auto-organisation) et d√©velopp√© des *soft skills* en communication et r√©solution de probl√®mes. Le projet a d√©montr√© la capacit√© √† concevoir une **architecture robuste et modulable**, r√©pondant aux exigences de s√©curit√© (*chiffrement, OTP, QR code*) en environnement professionnel.

Plusieurs **difficult√©s** ont √©t√© rencontr√©es (prises en main de la CI/CD OpenFaaS, configuration des secrets, d√©ploiement sous K8s local), g√©r√©es par des *solutions de repli planifi√©es* (ex. migration rapide vers Minikube/Docker en cas de probl√®me Kubernetes). La **m√©thode agile** a permis de r√©agir aux impr√©vus (absences ou retards) en r√©affectant les t√¢ches critiques et en ajustant le p√©rim√®tre du sprint.

En termes d'**impact pour l'entreprise**, ce Proof of Concept constitue une *base solide* pour un d√©ploiement ult√©rieur sur une infrastructure cloud publique ou hybride. Il r√©pond directement au **besoin m√©tier** : s√©curiser les acc√®s cloud de Cofrap, automatiser la gestion des identifiants et diminuer les co√ªts de support. La solution modulaire pourra √™tre √©tendue (int√©gration √† un SSO, mont√©e en charge, etc.) et prouve que l'√©quipe poss√®de les **comp√©tences techniques et organisationnelles** attendues d'un expert SI.

## II. Valorisation des comp√©tences

### B. Analyse des comp√©tences par bloc de certification

#### 1. Analyser et d√©finir la strat√©gie d'un syst√®me d'information

##### 1.1 Description des comp√©tences vis√©es

- **Mettre en place une veille technologique**

Nous mettons en place un **dispositif de veille technologique** en fran√ßais et en anglais, orient√© sur les besoins des m√©tiers de l'entreprise. Cette d√©marche nous permet d'anticiper les *technologies √©mergentes* comme l'**intelligence artificielle, l'IoT, la robotique** ou encore la **blockchain**. Nous s√©lectionnons, synth√©tisons et restituons les r√©sultats de cette veille aux d√©cideurs afin d'orienter la strat√©gie du SI vers les solutions les plus *prometteuses* et les plus *adapt√©es* √† nos enjeux.

- **Collecter et analyser les besoins m√©tiers**

Nous recueillons et analysons les **besoins exprim√©s par les directions m√©tiers** pour b√¢tir le projet de d√©veloppement du syst√®me d'information. Cette collecte s'effectue par des *entretiens, des ateliers de travail* ou l'√©tude de processus existants. L'objectif est de garantir que l'√©volution du SI respecte les **attentes op√©rationnelles** tout en assurant un *alignement strat√©gique* avec les orientations globales de l'entreprise.

- **Analyser la strat√©gie de l'entreprise et diagnostiquer le SI**

Nous analysons la **strat√©gie g√©n√©rale de l'entreprise** en √©tudiant son *environnement concurrentiel*, son march√© et son mode de fonctionnement. √Ä partir de cette analyse, nous √©tablissons un **diagnostic du syst√®me d'information** en place. Ce diagnostic nous permet de mettre en √©vidence les *forces et les faiblesses* du SI et de mesurer son ad√©quation avec les **objectifs strat√©giques** de l'organisation.

- **Cartographier un syst√®me d'information existant**

Nous cartographions le syst√®me d'information existant selon ses **quatre niveaux** : *m√©tier, fonctionnel, applicatif et infrastructure*. Cette cartographie d√©taill√©e nous apporte une **vision globale et structur√©e** de l'ensemble des composants du SI. Elle constitue un outil essentiel pour identifier les *points de convergence*, les doublons √©ventuels et les zones n√©cessitant une √©volution.

- **Identifier les informations sensibles et les risques**

√Ä partir de la cartographie r√©alis√©e, nous identifions les **informations sensibles** de l'entreprise ainsi que les *zones critiques* et les chemins d'attaque possibles. Cette analyse nous permet de contribuer √† la **politique de s√©curit√©** du SI en fournissant au *Responsable de la S√©curit√© des Syst√®mes d'Information (RSSI)* une vision claire des risques et des **priorit√©s de protection**.

- **√âlaborer la strat√©gie informatique**

Nous √©laborons la **strat√©gie informatique** de l'entreprise en prenant en compte ses objectifs, sa strat√©gie g√©n√©rale et son *sch√©ma directeur*. Nous proposons des projets d'**√©volution, d'adaptation ou de migration** du syst√®me d'information afin d'accompagner le d√©veloppement global de l'organisation, tout en r√©pondant aux besoins croissants en *cybers√©curit√©* et en *conformit√© r√©glementaire*.

- **Pr√©senter et prioriser les projets SI**

Nous pr√©parons des propositions de projets d'√©volution du syst√®me d'information que nous pr√©sentons au comit√© de direction. Ces projets sont prioris√©s afin de s'assurer que les plus strat√©giques soient lanc√©s en premier, en coh√©rence avec la strat√©gie d√©finie et les capacit√©s budg√©taires et organisationnelles de l'entreprise.

- **D√©finir des indicateurs de performance (KPI)**

Nous d√©finissons des indicateurs cl√©s de performance en nous appuyant sur des m√©thodes reconnues comme les SLA, ITIL ou encore le TRS. Ces indicateurs alimentent des tableaux de bord qui nous permettent de mesurer la performance du SI, de suivre son √©volution et de d√©tecter rapidement des axes d'am√©lioration.

- **Am√©lioration continue par le Lean IT**

Nous d√©finissons √©galement des indicateurs de performance op√©rationnelle en nous appuyant sur les principes du Lean IT. Cette approche nous permet d'identifier les gaspillages, de r√©duire les co√ªts informatiques et d'am√©liorer la satisfaction des utilisateurs en proposant des √©volutions concr√®tes du SI.

- **Int√©grer les enjeux Green IT et RSE**

Nous int√©grons dans la strat√©gie du syst√®me d'information des indicateurs li√©s au Green IT et √† l'informatique responsable. En √©valuant l'empreinte √©nerg√©tique et carbone de nos infrastructures, nous proposons des pistes d'am√©lioration continue permettant de r√©duire l'impact environnemental et d'inscrire l'entreprise dans une d√©marche RSE plus responsable.

- **Piloter les processus avec le BPM**

Enfin, nous appliquons les composants du Business Process Management (BPM) pour assurer le suivi du syst√®me d'information existant. Gr√¢ce √† des outils de pilotage des flux et de gestion des processus, nous pouvons suivre en continu la r√©alisation des activit√©s et proposer des am√©liorations visant √† accro√Ætre la performance et la fiabilit√© du SI.

##### 1.2 Mise en ≈ìuvre des comp√©tences √† travers les projets professionnels

Dans la mise en pratique du **Bloc 1** de la certification RNCP35584, nous nous sommes appuy√©s principalement sur le projet d'**optimisation du syst√®me d'information** (SI) de la PME **K-ElectroniK**, tout en citant √©galement les projets *Cofrap* ou *Nestor IA* √† titre illustratif. Ce projet visait √† **aligner le SI** sur la strat√©gie de croissance de K-ElectroniK (par exemple le d√©ploiement international d'un nouveau CRM pour soutenir les ventes). Conform√©ment aux comp√©tences requises, l'**analyse strat√©gique** a √©t√© la premi√®re √©tape. L'objectif √©tait d'¬´ √©laborer la strat√©gie informatique de l'entreprise en analysant les objectifs et la strat√©gie g√©n√©rale ¬ª de K‚ÄëElectroniK pour proposer des **√©volutions du SI** en coh√©rence avec son d√©veloppement. Cette d√©marche s'inscrit dans l'**alignement strat√©gique**, un levier essentiel de la performance d'entreprise : un SI bien align√© ¬´ soutient et renforce la performance ¬ª et garantit que les *investissements IT* sont en phase avec les **priorit√©s m√©tier**.

- **Diagnostic, cartographie et veille technologique**

Apr√®s cette phase strat√©gique, nous avons r√©alis√© un **diagnostic complet** du SI existant. Celui-ci a d√©but√© par une **cartographie multi-niveaux** (*m√©tier, applicatif, infrastructure*) de l'organisation et de son syst√®me d'information. Cette d√©marche d'*urbanisation du SI* vise √† ¬´ identifier l'ensemble des √©l√©ments du SI ¬ª pour disposer d'une **vision globale centralis√©e**. Nous avons ainsi mod√©lis√© les **processus m√©tier clefs** en *BPMN* (par exemple la gestion des campagnes marketing) et repr√©sent√© les applications et serveurs en **ArchiMate**. Cette base de connaissance du syst√®me a permis de mettre en lumi√®re les **forces et faiblesses existantes** (redondances applicatives, obsolescence technique, etc.), comme le souligne Projexion : un diagnostic apporte un *regard ext√©rieur* pour identifier les points forts et les d√©ficiences du SI.

En parall√®le, un **dispositif de veille technologique** a √©t√© instaur√©. Nous avons surveill√© les *innovations* (**IA, IoT, blockchain**‚Ä¶) pertinentes pour K-ElectroniK et √©labor√© des **rapports p√©riodiques** √† destination du comit√© de direction. Cette veille a aliment√© les **recommandations strat√©giques** en proposant, par exemple, l'int√©gration de solutions √©mergentes (*chatbots* pour le support client, *automatisation* des entrep√¥ts) afin d'anticiper les besoins futurs de l'entreprise.



- **S√©curit√©, gestion des risques et PSSI**

La **s√©curit√© du SI** a √©t√© trait√©e √† travers une *analyse de risques* formalis√©e. Sur la base de la cartographie, nous avons list√© les **donn√©es sensibles**, les *zones critiques* et les *vecteurs d'attaque* potentiels, conform√©ment √† l'exigence de la certification d'¬´ identifier les informations sensibles, les risques, [et] les chemins d'attaque ¬ª pour aider √† d√©finir la **politique de s√©curit√©**. Les r√©sultats ont conduit √† l'√©laboration d'une **Politique de S√©curit√© du Syst√®me d'Information (PSSI)** pour K‚ÄëElectroniK. La PSSI est le *document de r√©f√©rence* en mati√®re de s√©curit√© : elle ¬´ d√©finit les objectifs √† atteindre et les moyens accord√©s pour y parvenir ¬ª et repose sur une **analyse pr√©alable des risques**. Pour K‚ÄëElectroniK, elle comporte notamment les *r√®gles de sauvegarde* et de *continuit√© d'activit√©* pour les fonctions critiques (finance, production), les **exigences de chiffrement** et *contr√¥le d'acc√®s*, ainsi qu'un **plan de reprise d'activit√©** d√©finissant les d√©lais maximal de r√©tablissement. Cette d√©marche assure que le SI est s√©curis√© de mani√®re coh√©rente avec les enjeux m√©tier et r√©glementaires (*RGPD, normes ISO 27001/27005*).

- **Indicateurs de performance, Lean IT et Green IT**

Afin d'assurer le **pilotage de la performance** du SI, nous avons d√©fini des **indicateurs cl√©s** sur plusieurs dimensions. D'une part, des *m√©triques de qualit√© de service* (**SLA**, taux d'incidents, *TRS* des syst√®mes critiques) ont √©t√© √©tablies et visualis√©es dans un **tableau de bord ITIL**. Cela permet de suivre en temps r√©el la *disponibilit√© des services*, le respect des engagements contractuels et d'identifier rapidement les d√©rives. D'autre part, inspir√©s par une approche **Lean IT**, nous avons introduit des **indicateurs op√©rationnels** ax√©s sur les *co√ªts* et la *satisfaction utilisateur* (par exemple, le co√ªt total de possession des serveurs et la dur√©e de traitement d'un ticket). Cette d√©marche a mis en √©vidence des **leviers d'am√©lioration** (*streamlining* des processus IT, redimensionnement du parc applicatif) pour r√©duire les gaspillages et optimiser les ressources. Enfin, dans une perspective de **SI responsable**, des indicateurs **Green IT** ont √©t√© int√©gr√©s : *consommation √©nerg√©tique* des data centers, *√©missions carbone* du parc informatique, *taux de recyclage* des √©quipements. Ces **indicateurs RSE** permettent √† K‚ÄëElectroniK de mesurer l'impact environnemental de son SI et de mettre en place des *pratiques vertes* (virtualisation des serveurs, politique de recyclage, t√©l√©travail) pour limiter l'**empreinte √©cologique** de son informatique.

- **Gouvernance, m√©thodologie et livrables**

Le **pilotage du projet** s'est effectu√© en mode *gouvernance partag√©e* entre DSI, directions m√©tiers et CODIR. Des **ateliers r√©guliers** ont r√©uni le RSSI, les chefs de projet m√©tier (*logistique, finance, marketing*), la DAF et la direction g√©n√©rale pour valider les hypoth√®ses et prioriser les actions. Les **√©volutions propos√©es** (renforcement du CRM, rationalisation applicative, etc.) ont √©t√© pr√©sent√©es au comit√© de direction et prioris√©es en fonction de leur **alignement strat√©gique**. Par exemple, la *mont√©e en puissance du CRM* a √©t√© consid√©r√©e prioritaire pour soutenir l'expansion commerciale, tandis que des travaux d'*optimisation d'infrastructure* ont √©t√© planifi√©s apr√®s-coup. En termes de m√©thodologie, nous avons coupl√© des **approches structur√©es** (planification en phases avec cahiers des charges fonctionnel et technique) et **agiles** (*sprints it√©ratifs* pour livrer rapidement des am√©liorations incr√©mentales) afin de concilier rigueur et flexibilit√©.

Les **livrables produits** sont vari√©s : rapport d'audit et de diagnostic du SI, *cartographies As-Is/To-Be*, matrices de risques et plan de traitement, canevas de PSSI, cahiers des charges technique et fonctionnel, plan de transformation prioris√©, ainsi que **tableaux de bord** de suivi des indicateurs. Les **outils utilis√©s** ont notamment √©t√© : *Archi* et *Sparx EA* pour la mod√©lisation ArchiMate/BPMN, *Excel/Power BI* pour les tableaux de bord, et des plateformes collaboratives (*Jira, Confluence*) pour la gestion du projet et la capitalisation documentaire. Ces livrables et outils ont permis d'impliquer l'ensemble des **parties prenantes** (m√©tier, IT interne et prestataires externes) et de s√©curiser la d√©marche technique et organisationnelle.

- **Apports professionnels et conclusion**

En mobilisant ce large spectre de **comp√©tences du Bloc 1**, j'ai consolid√© ma *vision syst√©mique* et *strat√©gique* du SI. Ce travail a permis √† K-ElectroniK d'obtenir une **feuille de route SI coh√©rente**, s√©curis√©e et durable, contribuant directement √† sa performance op√©rationnelle. Comme le souligne Abraxio, la DSI est un √©l√©ment cl√© de la **comp√©titivit√© de l'entreprise** : ¬´ une entreprise qui n'arrive pas √† aligner ses technologies avec ses objectifs strat√©giques peut perdre en comp√©titivit√© ¬ª. √Ä l'inverse, un **alignement r√©ussi** ¬´ maximise l'efficacit√© de l'IT tout en contribuant activement aux objectifs strat√©giques ¬ª. Gr√¢ce √† cette exp√©rience, j'ai d√©velopp√© des **m√©thodes op√©rationnelles** (*cartographie d'urbanisme, diagnostic syst√©mique, √©laboration de PSSI, d√©finition de KPI Lean/Green*) et renforc√© ma capacit√© √† dialoguer aux niveaux strat√©giques (*CODIR*) comme op√©rationnels. Ces **apports professionnels** font d√©sormais partie int√©grante de ma pratique : je ma√Ætrise la conduite de *diagnostics SI complexes* et la traduction de l'analyse strat√©gique en **projets technologiques** align√©s sur la strat√©gie de l'entreprise.

#### 2 Manager un projet informatique avec agilit√© en collaboration avec les parties prenantes

##### 2.1 Description des comp√©tences vis√©es

- **Planifier le projet and allouer les ressources**

Nous planifions l'ensemble des √©tapes d'un projet en d√©finissant les **livrables attendus** et en r√©partissant les t√¢ches selon les *comp√©tences disponibles*. Cette organisation rigoureuse nous permet d'anticiper les **besoins humains, techniques et financiers**, de ma√Ætriser les d√©lais et de limiter les *risques de d√©rive*.

- **R√©diger le cahier des charges and cadrer les besoins**

Nous concevons et r√©digeons les **cahiers des charges fonctionnels et techniques** afin de formaliser clairement les besoins m√©tiers. Ces documents, *valid√©s par les parties prenantes*, servent de r√©f√©rence pour guider l'√©quipe et garantir que les solutions d√©velopp√©es r√©pondent aux **attentes exprim√©es**.

- **Piloter le projet avec les m√©thodes agiles**

Nous adoptons les **principes et outils agiles** (*Scrum, Kanban*) pour d√©couper le projet en *it√©rations courtes* et livrer des **incr√©ments r√©guliers**. Les *sprints, m√™l√©es quotidiennes* et *revues de sprint* nous permettent d'ajuster rapidement le p√©rim√®tre et de r√©duire les d√©lais de mise en production tout en favorisant la **satisfaction du client**.

- **Suivre la performance et contr√¥ler l'avancement**

Nous mettons en place des **tableaux de bord** et des *indicateurs de suivi* (*burndown charts, KPIs qualit√© et budget*) afin de mesurer l'√©tat d'avancement. Ces outils nous aident √† d√©tecter les √©carts, √† **anticiper les risques** et √† d√©clencher les *actions correctives* n√©cessaires pour maintenir le projet sous contr√¥le.

- **Coordonner les prestataires externes**

Nous assurons le **pilotage des fournisseurs et sous-traitants** impliqu√©s dans le projet. Nous d√©finissons leurs engagements, suivons leurs *livrables* et veillons √† leur int√©gration dans le **planning global**. Cette coordination s√©curise la mise en ≈ìuvre technique et r√©duit les *risques de retard* ou de *non-conformit√©*.

- **Animer et accompagner l'√©quipe projet**

Nous animons l'√©quipe projet en instaurant les **principes de l'agilit√©** : *auto-organisation, coop√©ration* et *am√©lioration continue*. Nous encourageons la participation de chacun, facilitons la r√©solution des blocages et favorisons la **motivation collective** √† travers des *r√©trospectives* et des *rituels agiles* r√©guliers.

- **Communiquer avec une √©quipe multiculturelle**

Nous adaptons nos modes de communication aux **cultures et aux langues** des membres de l'√©quipe. Nous veillons √† utiliser des supports *compr√©hensibles par tous*, √† synchroniser les √©changes malgr√© les *fuseaux horaires* et √† garantir l'**inclusion de chacun** dans le processus de d√©cision.

- **Pr√©venir et g√©rer les conflits**

Nous favorisons un **climat de travail collaboratif** et anticipons les tensions potentielles. En cas de d√©saccord, nous jouons un r√¥le de *m√©diateur* en clarifiant les attentes, en recentrant les discussions sur les **objectifs du projet** et en transformant la *diversit√© culturelle* en levier d'innovation.

- **Organiser la communication quotidienne**

Nous mettons en place des **points d'√©change r√©guliers**, tels que les *r√©unions quotidiennes*, et utilisons des **outils collaboratifs** (*Jira, Teams, Slack, Trello*) pour synchroniser le travail de l'√©quipe. Ce *suivi quotidien* fluidifie la coordination et permet d'ajuster rapidement les priorit√©s.

- **Animer des r√©unions √† distance**

Nous animons des **r√©unions virtuelles dynamiques** en utilisant des *outils interactifs* (*visioconf√©rence, tableaux blancs num√©riques, sondages en direct*). Nous veillons √† impliquer chaque membre et √† rythmer les sessions pour maintenir l'**attention** et favoriser la *prise de d√©cision collective*.

- **Partager l'information et g√©rer la documentation**

Nous mettons en place des **espaces collaboratifs** (*SharePoint, Confluence, Drive*) pour centraliser les documents du projet. Cela garantit la *tra√ßabilit√©*, l'accessibilit√© et l'**actualisation permanente** des informations, facilitant la continuit√© du travail m√™me en cas d'absence ou d'arriv√©e de nouveaux membres.

- **Accompagner le t√©l√©travail et maintenir la motivation**

Nous accompagnons l'√©quipe dans l'adoption du **t√©l√©travail** en promouvant des *pratiques manag√©riales bienveillantes*. Nous veillons au respect de l'**√©quilibre vie professionnelle/vie personnelle**, instaurons des *rituels conviviaux* et valorisons les r√©ussites afin de maintenir une **motivation forte** et un *engagement durable*.

- **Anticiper les √©volutions du pilotage de projet**

Nous int√©grons les **√©volutions du pilotage moderne** : adoption des *pratiques DevOps* pour rapprocher d√©veloppement et exploitation, utilisation croissante de l'**UX/UI** pour placer l'utilisateur au centre, *cycles de d√©veloppement* toujours plus courts, et recours progressif √† l'**intelligence artificielle** pour pr√©dire les risques et optimiser les d√©cisions.

#### 2.2 Mise en ≈ìuvre des comp√©tences √† travers les projets professionnels

Dans le cadre du **Bloc 2**, nous avons appliqu√© la gestion de projet agile principalement lors du projet **Cofrap** (MSPR 2), tout en mobilisant des savoir-faire similaires sur d'autres projets (p. ex. *Nestor IA*). D√®s la phase de cadrage, un **cahier des charges initial** a √©t√© r√©dig√© pour formaliser les besoins m√©tier, les attentes utilisateurs et les contraintes techniques du projet. Ce document, bien que moins contraint qu'en m√©thode ¬´ forfait ¬ª, a servi √† clarifier la **vision du projet**, √† estimer un budget pr√©liminaire et √† fixer des objectifs mesurables avant de lancer le d√©veloppement. En mode agile, cette d√©marche cohabite avec la constitution d'un **product backlog** de *user stories* : le cahier des charges initial a permis de donner un cadre aux parties prenantes (*sponsor interne, clients, √©quipe de ma√Ætrise d'≈ìuvre*) et d'alimenter la planification, tout en sachant que son contenu pouvait √©voluer au fil des sprints.

- **Planification agile et rituel Scrum**

La **planification agile** du projet a √©t√© structur√©e selon un *framework Scrum* adapt√©. Nous avons mis en place des **it√©rations courtes** (*sprints* de deux semaines) avec un *Sprint Planning* pour s√©lectionner les fonctionnalit√©s √† r√©aliser. Au quotidien, l'√©quipe tenait un **Daily Stand-up** (*r√©union de synchronisation*) pour faire le point sur l'avancement des t√¢ches, remonter les probl√®mes et coordonner l'entraide. Un **tableau Kanban** (physique et dans *JIRA*) affichait en temps r√©el l'√©tat des *user stories* du sprint, facilitant la **visualisation collective** de l'avancement. Au terme de chaque sprint, nous organisions une **r√©trospective** pour analyser les r√©sultats (*graphique burn-down, v√©locit√©*, etc.) et ajuster notre fa√ßon de travailler. Ces **rituels agiles** ont permis de maintenir un rythme r√©gulier de livraison incr√©mentale : chaque sprint aboutissait √† un *incr√©ment fonctionnel* livr√© aux parties prenantes pour validation interm√©diaire. Les c√©r√©monies et la documentation (*backlog, d√©finition de ¬´ done ¬ª, plans de test*) ont donc rythm√© le projet et assur√© la **transparence** entre tous les acteurs.

- **Suivi de projet et outils de pilotage**

Le **suivi du projet** s'est appuy√© sur des *indicateurs agiles* et des outils de pilotage d√©di√©s. Nous avons principalement utilis√© **JIRA** pour g√©rer le backlog, le suivi des tickets et des t√¢ches. JIRA, con√ßu pour les √©quipes Agile, offrait des **mod√®les pr√™ts √† l'emploi** (*tableaux Scrum/Kanban*) et des rapports agiles complets. Par exemple, le **burndown chart** g√©n√©r√© automatiquement dans JIRA visualisait le nombre de *story-points* restants √† chaque sprint, ce qui nous aidait √† estimer la **v√©locit√© de l'√©quipe** et √† anticiper la charge de travail. En parall√®le, **Trello** a √©t√© employ√© comme outil compl√©mentaire : sa flexibilit√© et sa simplicit√© ont facilit√© la **planification visuelle** des t√¢ches transverses, accessible √† tous les membres de l'√©quipe, m√™me non-d√©veloppeurs. Des **tableaux de bord** (*dashboards*) personnalis√©s dans JIRA et Confluence affichaient en continu l'√©tat d'avancement (*taux d'ach√®vement des user stories, nombre de tickets ouverts/ferm√©s, points restants*), fournissant aux parties prenantes internes et externes une **vue d'ensemble** du projet.

- **Communication et coordination des parties prenantes**

La **communication et la coordination** des parties prenantes ont √©t√© des aspects cl√©s du projet. Nous avons institu√© des **points de suivi r√©guliers** avec les commanditaires et utilisateurs internes (*r√©unions hebdomadaires, d√©monstrations interm√©diaires*) pour leur pr√©senter les livrables du sprint et ajuster les priorit√©s. Au sein de l'√©quipe, chaque **r√¥le √©tait clairement d√©fini** : le *Product Owner* repr√©sentait la voix du client et des utilisateurs finaux, d√©finissant les besoins et priorisant les *user stories*. De son c√¥t√©, le **Scrum Master** assurait le bon fonctionnement de la m√©thode agile, facilitant les √©changes au sein de l'√©quipe et aidant √† lever les obstacles sans imposer de solutions. Cette organisation a contribu√© √† une **communication agile efficace** : on privil√©giait le dialogue r√©gulier, la concertation et l'acceptation du changement. Les outils de *visioconf√©rence* (*Teams, Zoom*) et la **messagerie instantan√©e** (*Slack*) ont aussi jou√© un r√¥le important pour maintenir le lien, surtout en situation de travail hybride.

- **Gestion des conflits en environnement agile**

Dans un environnement agile **auto-organis√©**, la gestion des conflits entre membres a √©galement √©t√© g√©r√©e de mani√®re collaborative. Conscients que les d√©saccords sont ¬´ *in√©vitables* ¬ª dans une √©quipe autog√©r√©e, nous avons veill√© √† les r√©soudre par la **m√©diation** et la *communication bienveillante*, en nous appuyant sur la posture du Scrum Master. Par exemple, en cas de conflit sur la priorisation ou sur un choix technique, nous convions rapidement les personnes concern√©es (souvent le *PO* et les d√©veloppeurs) √† une **r√©union d√©di√©e** pour clarifier les enjeux. On utilisait alors les techniques de **Communication Non Violente** adapt√©es √† l'agilit√© pour co-construire une solution, comme le pr√©conise la d√©marche *CNV-A*. Les **r√©trospectives de sprint**, o√π chaque membre peut exprimer ses points de vue sur le d√©roulement du projet, ont aussi servi d'espace pour d√©samorcer les tensions et am√©liorer les **interactions** au sein de l'√©quipe.

- **Pilotage documentaire et livrables**

Le **pilotage documentaire** a accompagn√© toutes ces phases. Nous avons aliment√© un **r√©f√©rentiel de projet** sur *Confluence* comprenant les sp√©cifications techniques, l'architecture d√©taill√©e, les comptes-rendus de r√©unions et les rapports de recette. Cette **mise √† jour r√©guli√®re** de la documentation √©tait essentielle : si le manifeste Agile pr√©f√®re un ¬´ *logiciel fonctionnel plut√¥t qu'une documentation exhaustive* ¬ª, rien ne justifie pour autant son absence totale. Au contraire, disposer de **documents clairs** (*cahier de recettes, manuel utilisateur, guides de configuration*) a permis de s√©curiser les transitions entre phases et les √©ventuels changements d'√©quipe. Ces **livrables** (cahier des charges, backlog, incr√©ments valid√©s, documents de recette et guides) ont constitu√© les √©l√©ments remis aux parties prenantes √† chaque jalon projet.

- **Apports professionnels et retours d'exp√©rience**

Les **apports professionnels** de ce bloc ont √©t√© nombreux. La pratique du projet Cofrap nous a permis de d√©velopper une **vision compl√®te** du m√©tier de manager de projet informatique agile : de la formalisation initiale des besoins jusqu'√† la livraison incr√©mentale de la solution, en passant par l'animation quotidienne d'une √©quipe pluridisciplinaire et la relation avec les parties prenantes. Nous avons affin√© nos **comp√©tences en planification** (*affectation des ressources, estimation en story-points*), appris √† utiliser concr√®tement des m√©thodes agiles (*Scrum/Kanban*) et des **indicateurs de suivi** (*burn-down, v√©locit√©*) pour √©valuer notre progression. Les **difficult√©s rencontr√©es** ont notamment port√© sur la priorisation des t√¢ches sous contrainte de d√©lais serr√©s, la coordination de membres en t√©l√©travail et la gestion du changement face √† des besoins √©volutifs. Pour les surmonter, la **communication transparente** et des points fr√©quents nous ont permis de r√©ajuster le p√©rim√®tre en continu. Les **retours d'exp√©rience** (*r√©trospectives*) ont soulign√© qu'une communication d'√©quipe r√©guli√®re et la documentation accessible √©taient d√©cisives pour rester align√© et r√©agir aux impr√©vus.

- **Conclusion et perspectives**

En conclusion, ce bloc de comp√©tences nous a permis de consolider notre **ma√Ætrise de la gestion de projet** informatique agile en mode collaboratif. Nous avons d√©velopp√© notre capacit√© √† **planifier un projet complexe** (*r√©daction de sp√©cifications et backlog*), √† **animer une √©quipe agile** (*tenue de c√©r√©monies Scrum, suivi via tableaux Kanban*) et √† faire collaborer efficacement l'ensemble des parties prenantes (internes comme la direction, externes comme les fournisseurs). Ces **exp√©riences de projet** (notamment Cofrap, mais aussi *Nestor IA, K-ElectroniK*, etc.) ont ainsi contribu√© √† notre professionnalisation : nous savons aujourd'hui tirer parti des **outils** (*JIRA, Trello, visio-conf√©rence, documentation collaborative*) et des m√©thodes agiles pour d√©livrer de la valeur logicielle tout en g√©rant proactivement communication et conflits, ce qui est la marque d'un **manager de projet informatique agile confirm√©**.

#### 3 Piloter l'informatique d√©cisionnelle d'un syst√®me d'information (Big Data & BI)

##### 3.1 Description des comp√©tences vis√©es

- **Recueillir les besoins en donn√©es des m√©tiers**

Nous identifions et analysons les **besoins en donn√©es** exprim√©s par les directions m√©tiers afin de d√©finir une *strat√©gie d√©cisionnelle coh√©rente*. Nous organisons des *ateliers, des entretiens* et des analyses de processus pour traduire ces besoins en **sp√©cifications exploitables**. Notre objectif est de fournir aux m√©tiers des informations *fiables, pertinentes* et align√©es sur leurs **enjeux strat√©giques**.

- **Concevoir l'architecture d√©cisionnelle**

Nous d√©finissons et faisons √©voluer l'**architecture Business Intelligence** de l'entreprise. Nous concevons les *entrep√¥ts de donn√©es, datamarts* et flux d'int√©gration n√©cessaires pour centraliser et structurer les donn√©es issues des syst√®mes sources. Nous veillons √† garantir la **performance, la s√©curit√© et la fiabilit√©** de cette architecture, qui devient le *socle du pilotage* de l'entreprise.

- **√âlaborer la strat√©gie Big Data**

Nous mettons en place une **strat√©gie Big Data** adapt√©e aux besoins de l'entreprise. Cela inclut la collecte, le stockage et le traitement de *volumes massifs et vari√©s* de donn√©es, en int√©grant des solutions de **data lake, de traitements distribu√©s** et de *streaming*. Nous exploitons ces donn√©es pour **g√©n√©rer de la valeur**, optimiser les processus et anticiper les besoins des clients.

- **D√©ployer des mod√®les statistiques et de data science**

Nous d√©veloppons et appliquons des **mod√®les statistiques** et des *algorithmes de machine learning* afin d'exploiter la donn√©e au-del√† du descriptif. Nous construisons des **analyses pr√©dictives et prescriptives** qui aident √† d√©tecter des opportunit√©s, anticiper les tendances et orienter les *d√©cisions strat√©giques*. Ces travaux sont men√©s en collaboration avec les m√©tiers pour garantir la **pertinence** et l'**appropriation** des r√©sultats.

- **Valoriser les donn√©es par la visualisation**

Nous concevons des **tableaux de bord, rapports** et outils de *datavisualisation* qui rendent les informations accessibles et compr√©hensibles pour tous les d√©cideurs. Nous adaptons nos visualisations aux **besoins sp√©cifiques** des utilisateurs et favorisons l'*interactivit√©* et la *clart√©* pour faciliter l'appropriation des donn√©es.

- **G√©rer les donn√©es de r√©f√©rence (Master Data)**

Nous d√©finissons et administrons les **donn√©es de r√©f√©rence** de l'entreprise, afin de garantir une *coh√©rence* et une *unicit√©* des informations utilis√©es par l'ensemble des services. Nous mettons en place des **r√©f√©rentiels centralis√©s** et des processus de validation pour assurer la fiabilit√© des *donn√©es strat√©giques* (clients, produits, fournisseurs).

- **Construire et administrer un entrep√¥t de donn√©es unifi√©**

Nous concevons et maintenons un **entrep√¥t de donn√©es unique** qui centralise les informations strat√©giques de l'entreprise. Cet entrep√¥t constitue la *"source de v√©rit√©"* partag√©e par l'ensemble des directions m√©tiers et garantit l'acc√®s rapide √† des informations **consolid√©es, fiables et historis√©es**.

- **Assurer la qualit√© et la fiabilit√© des donn√©es**

Nous veillons en continu √† la **qualit√© des donn√©es** en mettant en place des contr√¥les de *coh√©rence, de compl√©tude* et d'*exactitude*. Nous d√©ployons des **outils de data quality** et d√©finissons des indicateurs pour mesurer et am√©liorer la fiabilit√© des informations utilis√©es dans les analyses et les *prises de d√©cision*.

- **S√©curiser et assurer la conformit√© des donn√©es**

Nous appliquons les **politiques de s√©curit√©** et de gouvernance d√©finies par l'entreprise afin de prot√©ger la *confidentialit√©* et l'*int√©grit√©* des donn√©es. Nous veillons √† la conformit√© avec les r√©glementations en vigueur, notamment le **RGPD**, en int√©grant d√®s la conception des dispositifs de protection comme l'*anonymisation*, le *chiffrement* ou la *gestion fine des habilitations*.

##### 3.2 Mise en ≈ìuvre des comp√©tences √† travers le projet "Analyse des donn√©es √©lectorales"

Dans le cadre du projet **¬´ Analyse des donn√©es √©lectorales ¬ª**, nous avons mobilis√© l'ensemble des comp√©tences du Bloc 3, ax√©es sur le *Big Data* et la *Business Intelligence*. Ce projet, avait pour objectif de **collecter, traiter et analyser** des donn√©es massives sur les scrutins, de concevoir un syst√®me d√©cisionnel d√©di√©, puis de restituer les r√©sultats sous forme d'*insights exploitables* pour la prise de d√©cision. Concr√®tement, il s'agissait d'extraire des sources h√©t√©rog√®nes (r√©sultats √©lectoraux officiels, donn√©es socio-d√©mographiques) un **jeu de donn√©es unique**, de l'organiser dans un entrep√¥t analytique, d'y appliquer des techniques de *mod√©lisation statistique et pr√©dictive*, puis de cr√©er des visualisations claires pour les d√©cideurs. Ce travail a ainsi couvert toutes les √©tapes classiques d'un projet BI : l'**ETL** (Extract, Transform, Load), la mod√©lisation de donn√©es, la data visualisation, le contr√¥le qualit√© et la gouvernance des donn√©es (s√©curit√©, respect du RGPD), en suivant les bonnes pratiques du m√©tier.

- **Collecte et traitement des donn√©es (ETL)**

La phase initiale a consist√© √† **extraire et pr√©parer** les donn√©es √©lectorales issues de sources multiples (portails gouvernementaux, archives, API publiques). Nous avons mis en place un **processus ETL complet** pour ¬´ extraire, transformer et charger ¬ª les donn√©es dans un format homog√®ne. Comme le note Fernandez, les outils ETL ont ¬´ en charge cette fonction essentielle du syst√®me global d√©cisionnel. Il s'agit en effet de g√©rer toutes les √©tapes de la collecte et de la pr√©paration des donn√©es ¬ª. Concr√®tement, nous avons utilis√© des **scripts Python** (biblioth√®ques *Pandas/Spark*) pour extraire les fichiers bruts (*CSV, JSON, Excel*) des r√©sultats par bureau de vote, nettoyer les valeurs aberrantes (p. ex. noms de communes erron√©s), harmoniser les formats de date et les libell√©s, et calculer des indicateurs (*taux de participation, pourcentages par candidat*). Chaque √©tape de transformation (correction de doublons, imputation de donn√©es manquantes, standardisation des intitul√©s) a √©t√© **automatis√©e** pour garantir la reproductibilit√©. Nous avons veill√© √† programmer des **contr√¥les qualit√©** d√®s l'ETL : contraintes d'unicit√©, v√©rification de la coh√©rence des totaux (somme des bulletins). Cette collecte massive de donn√©es n√©cessitait de g√©rer des *flux importants* (plusieurs millions de lignes pour l'ensemble des communes fran√ßaises), en s'appuyant sur des scripts parall√©lis√©s pour maintenir les performances. Au final, l'ETL a permis de disposer d'un **jeu de donn√©es centralis√©**, propre et pr√™t √† l'analyse, pos√© en amont de l'entreposage d√©cisionnel.

- **Conception de l'entrep√¥t de donn√©es**

Afin de faciliter les analyses ult√©rieures, nous avons con√ßu un **entreposage de donn√©es** (data warehouse) structur√© selon un *sch√©ma en √©toile*. Les **tables de faits** contiennent les r√©sultats √©lectoraux (voix par candidat, abstentions, etc.) √† l'√©chelle de la commune et du tour de scrutin, li√©es √† plusieurs **tables de dimensions** (commune, d√©partement/r√©gion, cycle √©lectoral, parti politique). Cette mod√©lisation dimensionnelle (sch√©ma en √©toile) permet des **requ√™tes analytiques rapides** et intuitives. Comme l'explique la litt√©rature, ¬´ les entrep√¥ts de donn√©es‚Ä¶ sont con√ßus pour le traitement analytique en ligne (OLAP) et utilisent des techniques telles que la mod√©lisation dimensionnelle et les sch√©mas en √©toile pour faciliter les requ√™tes complexes sur de grands ensembles de donn√©es ¬ª. En pratique, nous avons utilis√© un **SGBD relationnel** (*PostgreSQL*) pour stocker cet entrep√¥t. L'√©quipe BI a configur√© les tables en fonction des besoins d'analyse, indexant les *cl√©s √©trang√®res* pour acc√©l√©rer l'acc√®s aux dimensions. Les **vues mat√©rialis√©es** ont √©t√© cr√©√©es pour calculer en amont certains agr√©gats fr√©quemment utilis√©s (total des voix par r√©gion, √©volution historique de la participation). Ce data warehouse fournit un **cadre unifi√©** : il offre ¬´ une vue d'ensemble du patrimoine de donn√©es de l'organisation, soutenant les fonctions strat√©giques de l'entreprise telles que la prise de d√©cision ¬ª. Par exemple, nous avons pu consolider en un seul endroit des donn√©es √©parses issues du minist√®re de l'Int√©rieur, de l'INSEE et d'enqu√™tes locales, donnant ainsi aux utilisateurs une **source unique et fiable** pour les analyses.

- **Mod√©lisation statistique et pr√©dictive**

Sur ce jeu de donn√©es unifi√©, nous avons appliqu√© des **mod√®les statistiques et pr√©dictifs** pour faire √©merger des tendances et anticiper des r√©sultats. Par exemple, l'objectif principal √©tait de mod√©liser la participation et la r√©partition des voix par candidat √† partir de *caract√©ristiques socio-d√©mographiques* (√¢ge moyen, revenu, taux de ch√¥mage, r√©sultats √©lectoraux pass√©s). Nous avons ainsi impl√©ment√© des **algorithmes de machine learning** (*r√©gression logistique multiclasse, for√™ts al√©atoires, r√©seaux de neurones*) √† l'aide de biblioth√®ques Python (*scikit-learn*). La mod√©lisation pr√©dictive ¬´ regroupe un ensemble de m√©thodes permettant de collecter et d'analyser des donn√©es d√©finies, de mani√®re √† les interpr√©ter pour en d√©duire des pronostics concernant des tendances futures, des √©v√©nements √† venir ¬ª. Conform√©ment √† ce principe, nous avons divis√© nos donn√©es en jeux d'**entra√Ænement et de test**, entra√Æn√© les mod√®les et mesur√© leur pr√©cision (*pr√©cision, rappel, courbe ROC*) sur le test. Par exemple, un mod√®le de classification multi-classes pour pr√©dire le parti arriv√© en t√™te dans chaque commune a atteint environ **80 % de pr√©cision** (bien au-del√† du hasard statistique) gr√¢ce aux variables explicatives choisies. Cette ¬´ vraisemblance des r√©sultats ¬ª est renforc√©e par le volume de donn√©es : ¬´ plus le nombre de donn√©es analys√©es est important, plus les r√©sultats des mod√®les de pronostics peuvent √™tre consid√©r√©s comme pr√©cis ¬ª. Les insights issus de ces mod√®les ont permis de cibler les **facteurs les plus d√©terminants** (√¢ge, niveau de dipl√¥me, secteurs industriels locaux), offrant des indications actionnables aux d√©cideurs sur les leviers d'influence √©lectorale.

- **Restitution et datavisualisation**

La phase finale a consist√© √† restituer les r√©sultats sous forme **visuelle accessible**. Nous avons d√©velopp√© des **tableaux de bord interactifs** (*Power BI/Tableau*) et des visualisations dynamiques (*courbes temporelles, histogrammes, cartes choropl√®thes*) pour illustrer les r√©sultats √©lectoraux et les pr√©dictions. Par exemple, une carte de la r√©gion Rh√¥ne pr√©sentait le taux de participation par canton, r√©v√©lant des **disparit√©s g√©ographiques marqu√©es**, tandis que des graphiques en barres mettaient en √©vidence les partis dominants par d√©partement. La datavisualisation a jou√© un **r√¥le cl√©** : elle a transform√© des donn√©es brutes en *insights exploitables* et tr√®s clairs. Comme le r√©sume Donald Lay (Charles Schwab Corp.), gr√¢ce aux tableaux de bord ¬´ nos tableaux de bord fournissent des insights exploitables et tr√®s clairs qui permettent d'aller de l'avant ¬ª. En effet, les visualisations ont rendu imm√©diatement visibles les **tendances** (par exemple, la corr√©lation entre faible participation et zones rurales) et point√© des anomalies (baisse soudaine de votes dans certaines communes), facilitant la discussion strat√©gique. De plus, la data visualisation permet de d√©tecter l'√©mergence de tendances et de ¬´ **prendre rapidement des d√©cisions** ¬ª en identifiant les probl√®mes sous-jacents. Par exemple, en visualisant en temps r√©el l'impact de param√®tres socio-√©conomiques sur les r√©sultats, l'√©quipe a pu r√©ajuster en cours de projet les hypoth√®ses et orienter la campagne vers les indicateurs les plus pertinents. Ainsi, les livrables graphiques ont v√©ritablement servi d'**outil d√©cisionnel** en exposant clairement l'information √† tous les acteurs concern√©s.

- **Qualit√© des donn√©es et gouvernance**

La qualit√© et la gouvernance des donn√©es ont √©t√© trait√©es avec la plus grande rigueur. Nous avons mis en place des **contr√¥les qualit√© automatiques** : validation de la coh√©rence interne (p. ex. somme des suffrages = nombre total de votants), d√©tection d'anomalies (*outliers*), et r√©conciliation contre des sources officielles de r√©f√©rence. L'objectif √©tait de garantir que ¬´ les donn√©es sont exactes, propres et facilement accessibles ¬ª, car la valeur des analyses d√©pend directement de la **qualit√© des donn√©es** entr√©es. De surcro√Æt, le projet impliquait des donn√©es potentiellement sensibles (dossiers √©lectoraux). Nous nous sommes strictement conform√©s aux r√®gles du **RGPD** : les informations individuelles n'√©taient pas utilis√©es, seuls des *chiffres agr√©g√©s* (totaux par commune) ont √©t√© trait√©s. Plus g√©n√©ralement, ¬´ les donn√©es collect√©es doivent l'√™tre de mani√®re licite, loyale et transparente; [‚Ä¶] utilis√©es √† des fins d√©termin√©es, explicites et l√©gitimes; [‚Ä¶] trait√©es de fa√ßon √† garantir une s√©curit√© appropri√©e ¬ª. Concr√®tement, nous avons document√© la **finalit√© de chaque champ** collect√©, chiffr√© les bases de donn√©es sensibles et limit√© les acc√®s √† l'environnement de traitement (*authentification et cryptage* en transit). Sur le plan de la gouvernance, un **dictionnaire de donn√©es** a √©t√© r√©dig√©, formalisant l'origine et la d√©finition de chaque attribut. De plus, des r√¥les clairs ont √©t√© √©tablis (*propri√©taire de donn√©es, responsable BI*) pour assurer le suivi permanent de la qualit√© et de la s√©curit√©. L'adoption de ces bonnes pratiques a permis de b√¢tir un **socle de donn√©es coh√©rent**, fiable et s√©curis√© : l'information ainsi gouvern√©e maximise sa valeur en limitant les risques d'erreur ou de non-conformit√©, tout en renfor√ßant la confiance des utilisateurs dans les analyses.

- **Bonnes pratiques op√©rationnelles et reproductibilit√©**

Tout au long du projet, nous avons respect√© les **normes et bonnes pratiques** m√©thodologiques courantes. Le code et les scripts ont √©t√© g√©r√©s en **contr√¥le de version** (*Git*) et soigneusement comment√©s. Nous avons fix√© les versions des logiciels (*Python, biblioth√®ques*) pour assurer la **reproductibilit√©** des traitements. Par exemple, un fichier README d√©taille exactement les √©tapes d'ex√©cution (installations, commandes) pour reconstituer l'ETL et la mod√©lisation sur une nouvelle machine. Cette transparence de la documentation √©tait essentielle : ¬´ fournir des explications et une documentation claire sur les m√©thodes, les hypoth√®ses et les limites favorise la confiance, la compr√©hension et la prise de d√©cision √©clair√©e ¬ª. Enfin, des **backups r√©guliers** et un pipeline automatis√© ont √©t√© mis en place : chaque nuit, un script assurait l'import quotidien des nouvelles donn√©es √©lectorales dans le data warehouse. Ainsi, le projet restait √† jour et reproductible d'un cycle √©lectoral √† l'autre, et toute nouvelle it√©ration pouvait d√©marrer imm√©diatement sans perte de savoir-faire.

- **Difficult√©s rencontr√©es**

Plusieurs d√©fis ont marqu√© cette exp√©rience. D'une part, l'**h√©t√©rog√©n√©it√© initiale** des donn√©es (formats vari√©s, r√®gles de nommage disparates entre sources) a n√©cessit√© beaucoup de travail de pr√©traitement impr√©vu. Par exemple, la fusion de tables d√©partementales distinctes a r√©v√©l√© des incoh√©rences dans les *codes INSEE* des communes, obligeant √† recourir √† des scripts de nettoyage suppl√©mentaires. De plus, la collecte des **m√©tadonn√©es** (par ex. g√©olocalisation fine ou donn√©es socio-d√©mographiques) a parfois √©t√© incompl√®te ou verrouill√©e par des licences, ce qui a limit√© la dimension pr√©dictive du mod√®le. Sur le plan technique, l'apprentissage de **nouveaux outils** (*Spark, plateforme BI*) dans les d√©lais impartis a √©t√© exigeant. Enfin, ajuster les mod√®les pr√©dictifs dans un contexte changeant (donn√©es partielles en temps r√©el) a demand√© une **phase it√©rative importante**, avec des recalibrages r√©guliers pour prendre en compte de nouvelles donn√©es de test. Chaque difficult√© a toutefois √©t√© r√©solue par un travail m√©thodique : *revue de code en √©quipe*, consultations de documentations sp√©cialis√©es, et √©changes avec des experts (interview avec un statisticien local) ont aid√© √† surmonter ces obstacles.

- **Valeur ajout√©e et impact des livrables**

Les livrables produits ont apport√© une **valeur ajout√©e concr√®te** √† la ma√Ætrise du projet : tableaux de bord, rapports automatis√©s et mod√®les pr√©dictifs ont fourni des *insights exploitables* aux d√©cideurs. Par exemple, la visualisation du taux de participation a mis en lumi√®re des ¬´ **d√©serts √©lectoraux** ¬ª m√©connus, permettant aux autorit√©s locales de cibler les campagnes d'information. Le mod√®le pr√©dictif, avec sa pr√©cision valid√©e, a offert une **estimation anticip√©e** de la participation au prochain scrutin (pr√©cision ~80 %), contribuant √† la planification logistique (gestion des bureaux de vote, estimation des bulletins n√©cessaires). Les visualisations interactives ont rendu accessibles des **tendances complexes** ‚Äì comme la corr√©lation entre niveau d'√©ducation et vote pour un parti ‚Äì qui n'auraient pas √©t√© perceptibles sans analyse BI. En ce sens, la BI d√©ploy√©e a nourri la **prise de d√©cision** : selon Splunk, une gouvernance ad√©quate (qualit√©, s√©curit√©) des donn√©es ¬´ am√©liore la qualit√© des donn√©es, r√©duit les risques, et renforce les capacit√©s de prise de d√©cision ¬ª. Nos dashboards ont ainsi permis aux utilisateurs de poser de nouvelles **questions strat√©giques** bas√©es sur les donn√©es (ex. ¬´ quelles actions pour augmenter la participation en zones urbaines ? ¬ª), au lieu de s'en remettre √† l'intuition seule.

- **Conclusion et perspectives**

En conclusion, ce projet a pleinement illustr√© l'importance et la port√©e des **comp√©tences du Bloc 3**. Professionnellement, j'ai consolid√© ma ma√Ætrise des **processus Big Data/BI complets** ‚Äì de la collecte √† la restitution ‚Äì et appris √† orchestrer les diff√©rents outils (*ETL, SGBD, biblioth√®ques analytiques, plateformes BI*) au service de la d√©cision. J'ai √©galement d√©velopp√© mon sens critique quant aux **enjeux √©thiques et r√©glementaires** (s√©curit√©, RGPD) li√©s aux donn√©es sensibles. Sur le plan personnel, cette exp√©rience m'a apport√© de la rigueur et un esprit d'analyse renforc√© : j'ai appris √† documenter syst√©matiquement chaque √©tape pour assurer la **tra√ßabilit√©**, √† collaborer efficacement en √©quipe, et √† pr√©senter des r√©sultats de fa√ßon claire aux parties prenantes. √Ä l'avenir, le domaine de la BI et des donn√©es continue d'√©voluer rapidement : l'essor de l'**intelligence artificielle** et du *machine learning* exige de nouvelles comp√©tences. La gouvernance des donn√©es y jouera un r√¥le crucial, car elle ¬´ garantit la qualit√© et la fiabilit√© des donn√©es exploit√©es par les algorithmes de machine learning‚Ä¶ am√©liorant ainsi la pr√©cision et la fiabilit√© des r√©sultats ¬ª. Je me tiens donc pr√™t √† approfondir ces technologies (*analyse avanc√©e, cloud data platforms*, etc.) afin de faire √©voluer ma pratique professionnelle. Au final, l'ensemble des comp√©tences du Bloc 3 mobilis√©es ici ‚Äì du **Data Warehouse** √† la *Data Science*, en passant par la datavisualisation et la gouvernance ‚Äì a constitu√© un v√©ritable **levier de valeur ajout√©e**, aussi bien pour ce projet d'analyse √©lectorale que pour mon parcours professionnel futur en BI/Data.

#### 4 Concevoir et d√©velopper des solutions applicatives m√©tier et sp√©cifiques

##### 4.1 Description des comp√©tences vis√©es

- **Recueillir et analyser les besoins m√©tiers**

Nous recueillons les besoins aupr√®s des **utilisateurs et des directions m√©tiers** afin de comprendre leurs *processus, leurs contraintes* et leurs attentes. Nous menons des *ateliers, des interviews* et des observations pour traduire ces besoins en **sp√©cifications fonctionnelles claires**. Cette √©tape nous permet de garantir que la solution que nous allons d√©velopper r√©pondra r√©ellement aux usages et apportera une **valeur ajout√©e mesurable**.

- **Concevoir l'architecture applicative**

Nous d√©finissons l'**architecture de la solution** en tenant compte des enjeux de *performance, de s√©curit√©* et d'*√©volutivit√©*. Nous choisissons les mod√®les les plus adapt√©s (*monolithique, n-tiers, microservices, cloud-native*) et assurons une **int√©gration coh√©rente** avec le reste du syst√®me d'information. Notre objectif est de concevoir des solutions **robustes, r√©silientes** et capables de s'adapter aux *√©volutions futures* de l'entreprise.

- **D√©velopper les applications sur mesure**

Nous concevons et r√©alisons le **code source des applications** selon les *bonnes pratiques du g√©nie logiciel*. Nous utilisons des **langages et frameworks** adapt√©s au contexte (*Java, .NET, Python, JavaScript/TypeScript*, etc.) et appliquons les principes de qualit√© (**SOLID, tests unitaires, documentation**). Nous int√©grons d√®s le d√©veloppement les pr√©occupations de *s√©curit√©, d'ergonomie* et d'*accessibilit√©* afin de livrer des solutions **fiables et p√©rennes**.

- **Param√©trer et int√©grer les progiciels du march√©**

Nous mettons en ≈ìuvre des **solutions √©diteurs** (*ERP, CRM, outils m√©tiers*) en configurant et en adaptant leurs modules aux *processus internes* de l'entreprise. Nous effectuons les param√©trages n√©cessaires, d√©veloppons si besoin des **extensions sp√©cifiques** et int√©grons ces progiciels avec les autres composants du syst√®me d'information. Ce travail permet d'exploiter la puissance de solutions standard tout en les alignant avec les **besoins m√©tiers**.

- **Tester et valider la qualit√© logicielle**

Nous assurons l'ensemble des **phases de test** n√©cessaires √† la validation des applications : *tests unitaires, tests d'int√©gration, tests fonctionnels, tests de charge* et *tests de s√©curit√©*. Ces campagnes nous permettent de d√©tecter rapidement les anomalies, d'apporter des corrections et d'am√©liorer la **qualit√© du produit**. La validation finale est effectu√©e en collaboration avec les utilisateurs, dans le cadre de *recettes* qui confirment la conformit√© de la solution aux **attentes exprim√©es**.

- **Mettre en place l'int√©gration continue et le d√©ploiement automatis√©**

Nous d√©ployons des **pipelines d'int√©gration continue** et de *livraison continue* (**CI/CD**) afin d'automatiser la compilation, les tests et la mise en production des applications. Gr√¢ce √† ces pratiques, nous garantissons des **mises √† jour fr√©quentes, rapides et s√©curis√©es**, tout en r√©duisant les *risques d'erreur humaine*. L'int√©gration de *pratiques DevOps* nous permet de rapprocher d√©veloppement et exploitation pour am√©liorer la **r√©activit√©** et la **qualit√© globale** du service rendu.

- **Produire la documentation et former les utilisateurs**

Nous r√©digeons une **documentation claire** et adapt√©e aux *utilisateurs finaux* ainsi qu'aux *√©quipes techniques* charg√©es de la maintenance. Nous cr√©ons des *guides d'utilisation*, des *supports de formation* et des *documents techniques de r√©f√©rence*. Nous accompagnons les utilisateurs par des **sessions de formation** et un *support de proximit√©* afin de favoriser une **appropriation rapide et efficace** de la nouvelle solution.

- **Accompagner la conduite du changement**

Nous veillons √† ce que l'**adoption de la solution** par les √©quipes soit facilit√©e. Nous mettons en place un *plan de communication*, identifions et impliquons des *utilisateurs relais*, organisons des **formations adapt√©es** et restons disponibles lors du d√©ploiement pour accompagner la transition. Notre r√¥le est de r√©duire les *r√©sistances*, de rassurer et d'encourager l'**utilisation optimale** de l'outil, afin que la solution livr√©e s'int√®gre naturellement dans les *pratiques quotidiennes*.

#### 4.2 Mise en ≈ìuvre des comp√©tences √† travers les projets professionnels

Le projet **Nestor IA** (Proof of Concept de floutage automatique d'images) sert de fil directeur pour illustrer les comp√©tences du **bloc 4**. Son objectif est d'**anonymiser les individus** sur des photos (par exemple en floutant les visages), afin de respecter la *conformit√© RGPD*. Ce POC a ainsi n√©cessit√© une **analyse fonctionnelle approfondie** des exigences de protection des donn√©es, notamment que ¬´ l'*anonymisation irr√©versible* ¬ª d'une image rend les donn√©es hors du champ d'application du RGPD. √Ä partir de ce cadrage, nous avons con√ßu une **solution modulaire** : une architecture *monorepo* (via *Yarn Workspaces*) regroupant le frontend et plusieurs backends, une s√©paration claire *front-end / back-end*, et le support de **technologies multiples** (*Python* et *.NET* pour le serveur, *Next.js* pour l'interface utilisateur).

- **Conception de la solution**

L'**analyse des besoins** a d'abord port√© sur la *conformit√© RGPD* et la d√©finition des cas d'usage (types d'images, degr√©s de floutage, permissions utilisateurs). Cette √©tude a permis de sp√©cifier la **fonctionnalit√© cl√©** : d√©tecter et flouter automatiquement les visages (ou d'autres √©l√©ments sensibles) dans les images. Sur la base de ce cahier des charges, l'**architecture retenue** est un *monorepo JavaScript/Python/C#* avec *Yarn Workspaces*. Cette approche permet de regrouper plusieurs projets dans un seul d√©p√¥t git tout en partageant des **biblioth√®ques communes**. Elle √©vite la duplication de code (par exemple pour la gestion des configurations ou des types de donn√©es) et facilite la **r√©utilisation de composants** entre le frontend Next.js et les backends. La solution se compose de **trois briques principales** :

- **Back-end Python (Flask)** : impl√©mente une API REST pour l'inf√©rence du mod√®le de d√©tection et l'√©dition d'images. Flask est un *micro-framework Python* l√©ger et extensible, ¬´ con√ßu pour d√©marrer rapidement et pouvoir √©voluer vers des applications complexes ¬ª.
- **Back-end .NET (ASP.NET Core)** : impl√©mente en parall√®le une API REST √©quivalente en C#. ASP.NET Core est un *framework open-source, multiplateforme* pour cr√©er des applications web en C#. Il offre de tr√®s **hautes performances** (plusieurs millions de requ√™tes par seconde selon les benchmarks) et renforce la flexibilit√© du POC en supportant les environnements *Windows et Linux*.
- **Front-end Next.js** : r√©alise l'interface web de visualisation et de contr√¥le, en React avec *rendu c√¥t√© serveur* (SSR) ou g√©n√©r√© (SSG) pour de meilleures performances et SEO. Next.js facilite √©galement la **structure modulaire** du monorepo c√¥t√© client, en permettant par exemple de partager facilement des composants React entre diff√©rentes pages.

Chaque partie a son propre r√©pertoire dans le monorepo (g√©r√© par *Yarn Workspaces*), ce qui simplifie l'installation des d√©pendances et la **mise en commun de biblioth√®ques** (ex. composants UI ou fonctions utilitaires) entre le frontend et les backends. Les deux backends s'appuient sur un mod√®le de **microservices** : ils communiquent via des *API REST* bien d√©finies (*GET* pour r√©cup√©rer une image, *POST* pour envoyer une image √† flouter). Cette s√©paration *front/back* respecte les principes de conception de la solution applicative (**modularit√©, d√©couplage**).

- **D√©veloppement applicatif**

**Impl√©mentation Python (Flask, OpenCV, YOLOv8)**. Le backend Python utilise **Flask** pour exposer des *endpoints REST*. L'un des points forts est l'int√©gration du mod√®le de d√©tection **Ultralytics YOLOv8**. Ce framework offre une interface Python conviviale et permet d'exporter les mod√®les au format *ONNX* ¬´ with just a few lines of code ¬ª. Le mod√®le pr√©-entra√Æn√© (ou affin√© sur un jeu de donn√©es sp√©cifique) d√©tecte les visages dans l'image ; la librairie **OpenCV** est alors utilis√©e pour appliquer un *filtre de flou* sur les r√©gions d√©tect√©es. Par exemple, on charge une image en m√©moire, on ex√©cute `results = model.predict(source=im)` pour obtenir les bo√Ætes englobantes, puis on floute chaque zone avec OpenCV (*filtre gaussien*). Cette impl√©mentation exploite le **pipeline Python habituel** (pr√©traitement des images, passage dans YOLO, post-traitement, renvoi du r√©sultat).

**Impl√©mentation .NET (ASP.NET Core, ONNX Runtime, SkiaSharp)**. En parall√®le, un service √©quivalent a √©t√© d√©velopp√© en **C#**. On a import√© le mod√®le YOLOv8 en *ONNX* (voir section technique suivante) pour l'utiliser avec **ONNX Runtime**, permettant l'inf√©rence dans l'environnement .NET. Les images sont trait√©es avec la librairie **SkiaSharp** (*wrapper .NET* de la biblioth√®que *Skia*) pour le dessin et le floutage. Cette combinaison s'est r√©v√©l√©e efficace : selon devMobile, l'impl√©mentation SkiaSharp (NickSwardh V2) am√©liore significativement la **performance de traitement** compar√©e √† d'autres librairies d'image .NET (p. ex. *ImageSharp*), r√©duisant fortement le **temps de traitement**. Ce backend expose les m√™mes *endpoints REST* que le service Flask (on pouvait ainsi comparer directement les deux versions). La s√©paration des backends a n√©cessit√© de **synchroniser le mod√®le** et la logique m√©tier : par exemple, la m√™me taille d'image, la m√™me normalisation et le m√™me ordre de canaux (*RGB vs BGR*) ont d√ª √™tre appliqu√©s pour que les r√©sultats soient coh√©rents.

**Interface client (Next.js)**. Le frontend Next.js propose une **interface fluide** pour t√©l√©charger/visualiser les images, lancer le floutage et afficher le r√©sultat. Les appels aux backends (*Python* ou *.NET*) se font via des requ√™tes *AJAX* (*fetch*) vers les APIs REST. Next.js fournit un **rendu rapide c√¥t√© serveur** pour le chargement initial, puis une *application monopage* (SPA) pour les interactions, am√©liorant l'ergonomie. Gr√¢ce au monorepo, le **code UI** (par ex. composants React de visualisation d'image) est partag√© facilement entre les diff√©rentes pages du site.

- **Choix techniques et organisation**

Plusieurs **d√©cisions techniques** structurent la solution :

- **Export des mod√®les au format ONNX** : YOLOv8 permet d'exporter directement en *ONNX*. Ce format standard facilite l'**interop√©rabilit√©** (le m√™me mod√®le ONNX est utilis√© par *Python/Flask* et par *ONNX Runtime* en .NET). L'export a n√©cessit√© de g√©rer avec pr√©cision les √©tapes de *pr√©/post-traitement* (antialiasing au redimensionnement, normalisation dans la bonne plage, ordre des canaux) pour obtenir les m√™mes performances que le mod√®le original en *PyTorch*.

- **Architecture Monorepo (Yarn Workspaces)** : Le d√©p√¥t Git comporte plusieurs sous-projets (backends, frontend, mod√®les, scripts) g√©r√©s par *Yarn Workspaces*. Cela permet de partager facilement des biblioth√®ques communes et d'√©viter la duplication de code. Par exemple, les composants React et les fonctions utilitaires sont d√©finis une seule fois et utilis√©s √† la fois par le frontend et les backends.

- **API REST et microservices** : Chaque backend expose une API REST avec des *endpoints* clairs pour la d√©tection et le floutage. Cela permet une communication simple et efficace entre le front-end et les back-ends, tout en respectant les principes de **modularit√©** et de **d√©couplage**.

- **Utilisation de Flask et ASP.NET Core** : Flask a √©t√© choisi pour sa l√©g√®ret√© et sa rapidit√© de d√©veloppement en Python, tandis qu'ASP.NET Core offre des performances √©lev√©es et une grande flexibilit√© en C#. Ces choix technologiques permettent de tirer parti des forces de chaque langage et framework.

- **Tests et validation** : Des tests unitaires et d'int√©gration sont r√©alis√©s pour chaque composant (front-end et back-end) afin de garantir le bon fonctionnement de l'ensemble de la cha√Æne de traitement. Par exemple, des tests automatis√©s v√©rifient que le floutage est bien appliqu√© sur les zones d√©tect√©es comme sensibles.

- **S√©curit√© et conformit√© RGPD** : Des mesures sont mises en place pour garantir la s√©curit√© des donn√©es trait√©es et la conformit√© avec le RGPD. Par exemple, les images sont anonymis√©es avant tout traitement, et des contr√¥les d'acc√®s sont appliqu√©s aux APIs.

- **Pipeline CI/CD** : Un pipeline d'int√©gration et de livraison continues est configur√© pour automatiser le d√©ploiement des diff√©rentes composantes de la solution. Cela permet de garantir des mises √† jour rapides et fiables.

Chaque choix technique a √©t√© document√© et justifi√© dans le cadre du projet, en veillant √† ce que l'ensemble des parties prenantes ait une compr√©hension claire de l'architecture et des technologies utilis√©es.

- **Difficult√©s rencontr√©es et solutions**

Plusieurs **d√©fis techniques** ont √©t√© relev√©s :

- **Interop√©rabilit√© Python/.NET** : il a fallu faire communiquer deux langages diff√©rents. Pour cela, nous avons trait√© le syst√®me comme deux *microservices ind√©pendants* et fait dialoguer les API REST. Le d√©fi principal fut de maintenir la **coh√©rence des r√©sultats** : on a donc export√© le m√™me mod√®le en ONNX et reproduit exactement les pr√©traitements (cf. difficult√© de correspondance de normalisation de YOLOv8).

- **Gestion des mod√®les IA** : entra√Æner et exporter le mod√®le YOLO a n√©cessit√© de g√©rer les **d√©pendances d'Ultralytics** et d'adapter le code d'export (utiliser `model.export(format="onnx")`). On a rencontr√© des √©carts de performance li√©s au pr√©traitement, r√©solus en r√©pliquant les √©tapes du pipeline PyTorch (comme redimensionner avec *anti-aliasing* et normaliser sur [0,1]).

- **Optimisation du floutage** : tester diff√©rents *algorithmes de flou* (gaussien, flou mosa√Øque) pour trouver un bon compromis visuel/performance. L'int√©gration de *SkiaSharp* en .NET s'est av√©r√©e plus efficace que d'autres librairies .NET (p. ex. *ImageSharp*), r√©duisant fortement le **temps de traitement**.

- **Organisation monorepo** : bien g√©rer les d√©pendances partag√©es a n√©cessit√© d'utiliser *Yarn Workspaces* correctement (d√©finir les ¬´ workspaces ¬ª dans le `package.json` racine). Cela a r√©solu les **conflits de version** entre packages front-end et back-end et simplifi√© la maintenance des modules communs (par exemple, un module partag√© de validation d'image).

Chaque difficult√© a √©t√© l'occasion d'**apprentissage** et d'adoption de bonnes pratiques (gestion stricte des versions de mod√®le, profilage des performances, documentation des API, etc.).

- **Bilan professionnel et personnel**

Ce projet pivot a renforc√© de nombreuses **comp√©tences techniques et m√©thodologiques**. Professionnellement, j'ai approfondi la conception de *solutions monorepo* et *multi-technologiques*, appris √† manipuler des **mod√®les IA** dans des contextes industriels (*export ONNX, ONNX Runtime*) et d√©couvert l'√©cosyst√®me *ASP.NET Core* et *SkiaSharp*. J'ai acquis une exp√©rience pratique de **Flask** pour d√©velopper rapidement une API Python. Le projet a aussi sensibilis√© aux **bonnes pratiques RGPD** (anonymisation des donn√©es), √† la *qualit√© logicielle* (tests automatis√©s) et √† l'*int√©gration continue*.

Sur le plan personnel, j'ai d√©velopp√© mon **autonomie** et ma capacit√© √† r√©soudre des probl√®mes complexes (interop√©rabilit√© entre stacks, optimisation de performance). Ce POC a √©largi ma **culture de d√©veloppement** : conception de *RESTful APIs*, ma√Ætrise d'outils modernes (*Next.js* pour le frontend, *Yarn Workspaces* pour le monorepo), et mise en ≈ìuvre de techniques d'**IA appliqu√©e au m√©tier** (vision par ordinateur). Enfin, cette r√©alisation confirme l'importance de l'ouverture vers l'**IA m√©tier** : le floutage automatique, comme d'autres fonctionnalit√©s intelligentes, devient un atout m√©tier majeur. En somme, le projet Nestor IA illustre la mise en ≈ìuvre concr√®te des **comp√©tences du bloc 4** et apporte une valeur ajout√©e tant sur le plan professionnel (nouvelles comp√©tences acquises) que personnel (meilleure compr√©hension des enjeux IA et RGPD).